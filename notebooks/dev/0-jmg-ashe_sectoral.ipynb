{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the ASHE data\n",
    "\n",
    "ASHE is the Annual Survey of Hours and Earnings. It contains information about salaries in various regions and sectors of the UK. Here we want to calculate median salaries in Nesta sectors which we can then use to benchmark industries.\n",
    "\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "* Scrape the data\n",
    "* Parse it\n",
    "* Merge with out segments at the finest level of granularity possible\n",
    "* Create weighted median taking into account distribution of employment in the segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ashe_data(path,ons_path):\n",
    "    '''\n",
    "    Function to collect the ASHE data from the ONS website.\n",
    "    \n",
    "    Arguments:\n",
    "        path (str) is the path for the file we are interested in\n",
    "        ons_path (str) is the parent for all ashe files\n",
    "        \n",
    "    This will return a doanloaded and parsed file\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    file = requests.get(ons_path+path)\n",
    "    \n",
    "    #Create a zipfile with its content\n",
    "    z = ZipFile(BytesIO(file.content))\n",
    "    \n",
    "    #Extract names\n",
    "    names = z.namelist()\n",
    "    \n",
    "    #Select the names (they will meantion hourly gross but not the confidence intervals)\n",
    "    \n",
    "    my_name = [x for x in names if (all(names in x for names in ['Annual','Gross'])) & ('CV' not in x)]\n",
    "    \n",
    "    print(my_name)\n",
    "\n",
    "    #if len(my_name)>1:\n",
    "    #    print('Too many options')\n",
    "    #    break\n",
    "    \n",
    "    #Read into excel\n",
    "    infile = pd.read_excel(BytesIO(z.open(my_name[0]).read()),sheet_name=1,skiprows=4,\n",
    "                      na_values=['x','..',':'])\n",
    "    \n",
    "    #Drop missing values in the matching code or median (these don't interest us)\n",
    "    infile.dropna(axis=0,subset=['Code'],inplace=True)\n",
    "    \n",
    "    infile['Code'] = [x.strip() for x in infile['Code']]\n",
    "    \n",
    "    #container.append(infile.reset_index(drop=True))\n",
    "    \n",
    "    return(infile.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing files\n",
    "def add_zeros(container):\n",
    "    '''\n",
    "    This adds pre-zeroes to codes in categories A and B\n",
    "    \n",
    "    Args:\n",
    "        Container (df) is one of the dataframes we have created before\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    new_cont = container.copy()\n",
    "    \n",
    "    for pid,row in new_cont.iterrows():\n",
    "        \n",
    "        if row['Code']=='C':\n",
    "            break\n",
    "        else:\n",
    "            if row['Code'] not in ['A','B']:\n",
    "            \n",
    "            #print(row['Code'])\n",
    "                new_cont.loc[pid,'Code']='0'+row['Code']\n",
    "        \n",
    "    return(new_cont)\n",
    "    \n",
    "\n",
    "def year_ashe_lookups(ashe_table):\n",
    "    '''\n",
    "    \n",
    "    Takes an ashe table and outputs a list of code lookups depending on the level of resolution at which they are available\n",
    "    \n",
    "    Args:\n",
    "        ashe_table: an ashe table as above\n",
    "        \n",
    "    returns three dicts with code - salary lookups with decreasing levels of resolution\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Containers\n",
    "    ashe_4 = {}\n",
    "    ashe_3 = {}\n",
    "    ashe_2 = {}\n",
    "\n",
    "    #In each row it gets the length of a code (sic4,3 etc) and assigns the median salary to the right dict.\n",
    "    #We we will use this later to assign the median to \n",
    "    \n",
    "    for pid, row in ashe_table.iterrows():\n",
    "\n",
    "        code = row['Code'].strip()\n",
    "        med_sal = row['Median']\n",
    "\n",
    "        if len(code)==4:\n",
    "            ashe_4[code]= med_sal\n",
    "\n",
    "        elif len(code)==3:\n",
    "            ashe_3[code] = med_sal\n",
    "\n",
    "        elif len(code)==2:\n",
    "            ashe_2[code] = med_sal\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return([ashe_4,ashe_3,ashe_2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_salaries(lookup,four_digit, ashe):\n",
    "    '''\n",
    "    \n",
    "    Assigns each 4-digit sic code a median according to ASHE at its finest level of resolution\n",
    "    \n",
    "    Args:\n",
    "        lookup (df) a lookup with the 4 digit sic code we want to query against our ashe lookups\n",
    "        four_digit (str) the name of the variable with the four digits\n",
    "        ashe_lookups (list of dicts) the list of ashe code-median key-value pairs to query\n",
    "        \n",
    "    Returns\n",
    "        a table with four digit sics, names and salaries.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cont = []\n",
    "\n",
    "    #Is loo\n",
    "    for sic in lookup[four_digit]:\n",
    "        \n",
    "        if sic in ashe[0].keys():\n",
    "            #cont.append({sic:ashe_lookups[0][sic]})\n",
    "            cont.append([sic,ashe[0][sic]])\n",
    "            \n",
    "        elif sic[:-1] in ashe[1].keys():\n",
    "            \n",
    "            #cont.append({sic:ashe_lookups[1][sic[:-1]]})\n",
    "            cont.append([sic,ashe[1][sic[:-1]]])\n",
    "        \n",
    "        elif sic[:-2] in ashe[2].keys():\n",
    "            #cont.append({sic:ashe_lookups[2][sic[:-2]]})\n",
    "            cont.append([sic,ashe[2][sic[:-2]]])\n",
    "        \n",
    "        else:\n",
    "            #cont.append({sic:np.nan})\n",
    "            cont.append([sic,np.nan])\n",
    "    \n",
    "    return(pd.DataFrame(cont,columns=['sic_4','median_salary_thGBP']).set_index('sic_4'))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_path = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/industry4digitsic2007ashetable16/'\n",
    "\n",
    "#Ashe paths\n",
    "ashe_paths = ['2018provisional/table162018provisional.zip', '2017revised/table162017revised.zip',\n",
    "            '2016revised/table162016revised.zip','2015/table162015revised.zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashes = [get_ashe_data(p,standard_path) for p in ashe_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lost a few codes with zero at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashes[0].head()['Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_containers = [add_zeros(x) for x in ashes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pd.read_csv('../../data/aux/sic_4_industry_segment_lookup.csv',dtype={'sic_4':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing\n",
    "\n",
    "We need to assign salaries to their more detailed code. If a salary is available at the 4-digit then we are not interested in the salary at the 3 digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2 digit and four digit lookups from Ashe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ashe_lookups = [year_ashe_lookups(cont) for cont in new_containers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_salaries = pd.concat([map_salaries(cl,'sic_4',tab) for tab in all_ashe_lookups],axis=1)\n",
    "\n",
    "all_salaries.columns = [2018,2017,2016,2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_salaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create weighted medians\n",
    "\n",
    "We are not going to be working with sic codes, but our own segments. This requires creating weighted medians of salaries across SIC codes. We use levels of employment to create the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melt the salaries file from above\n",
    "salaries_long = all_salaries.reset_index(drop=False).melt(id_vars=['sic_4'],var_name='year',value_name='median_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We read for the four years\n",
    "bres_data = pd.concat([pd.read_csv(f'../../data/processed/official/nomis_BRES_{y}_TYPE450.csv',dtype={'SIC4':str}) for y in [2015,2016,2017,2018]],axis=0)\n",
    "\n",
    "bres_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group them by year to get the total level of employment by SIC4\n",
    "sic_yearly_long = bres_data.groupby(['year','SIC4'])['value'].sum().reset_index(drop=False)\n",
    "\n",
    "sic_yearly_long.rename(columns={'value':'employment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_empl_merge = pd.merge(salaries_long,sic_yearly_long,left_on=['sic_4','year'],right_on=['SIC4','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_merged = pd.merge(cl[['sic_4','cluster']],salary_empl_merge,left_on='sic_4',right_on='sic_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted salary: takes all the sics in a segment and applies a weight based on their importance in the segment\n",
    "weighted_sal = segment_merged.groupby(\n",
    "    ['cluster','year']).apply(lambda x: np.sum(x['median_salary']*x['employment'])/np.sum(x['employment'])).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashe_out = weighted_sal.rename(columns={0:'weighted_median_salary'})\n",
    "\n",
    "ashe_out.pivot_table(index='cluster',columns='year',values='weighted_median_salary').corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove some outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid,row in ashe_out.iterrows():\n",
    "    \n",
    "    if row['weighted_median_salary']<1000:\n",
    "    \n",
    "        ashe_out.loc[pid,'weighted_median_salary'] = np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate averages for all years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashe_out_grouped = pd.DataFrame(ashe_out.groupby(['cluster'])['weighted_median_salary'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashe_out_grouped['ashe_median_salary_rank'] = pd.qcut(ashe_out_grouped['weighted_median_salary'],np.arange(0,1.1,0.1),labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashe_out_grouped.sort_values('ashe_median_salary_rank',ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashe_out_grouped.to_csv(f'../../data/processed/official/{today_str}_ashe_rankings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
