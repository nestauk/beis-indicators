{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HESA \n",
    "\n",
    "Some code to collect HESA data and merge it with university metadata in order to create indicators about skills supply in the UK.\n",
    "\n",
    "We are interested in the following indicators:\n",
    "\n",
    "* Research staff: https://www.hesa.ac.uk/data-and-analysis/staff/working-in-he\n",
    "* Research spaces: https://www.hesa.ac.uk/data-and-analysis/estates/table-1\n",
    "* STEM graduates produced: https://www.hesa.ac.uk/data-and-analysis/students/what-study#\n",
    "* PhD graduates produced: this is also in https://www.hesa.ac.uk/data-and-analysis/students/what-study#\n",
    "\n",
    "\n",
    "See [this table](https://docs.google.com/spreadsheets/d/1V2fAQcvuLsoImwo6uLdyIK3x80pBNoX97CxsxkjvRP4/edit?usp=sharing) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from nuts_finder import NutsFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name,dirs = ['raw','processed']):\n",
    "    '''\n",
    "    Utility that creates directories to save the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for d in dirs:\n",
    "        if name not in os.listdir(f'../../data/{d}'):\n",
    "            os.mkdir(f'../../data/{d}/{name}')\n",
    "\n",
    "def tidy_cols(my_csv):\n",
    "    '''\n",
    "    Tidies column names ie lower and replace spaces with underscores\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return([re.sub(' ','_',col.lower()) for col in my_csv.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data,var_val_pairs):\n",
    "    '''\n",
    "    We use this to filter the data more easily than using pandas subsetting\n",
    "    \n",
    "    Args:\n",
    "        data (df) is a dataframe\n",
    "        var_val pairs (dict) is a dictionary where the keys are variables and the value are values\n",
    "\n",
    "    \n",
    "    '''\n",
    "    d = data.copy()\n",
    "    \n",
    "    for k,v in var_val_pairs.items():\n",
    "        d = d.loc[d[k]==v]\n",
    "        \n",
    "    return(d.reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categories(data,columns):\n",
    "    '''\n",
    "    This counts frequencies of categorical variables. We use it to decide what variables to choose, and to avoid double counting\n",
    "    \n",
    "    Args:\n",
    "        Data (df) is the data\n",
    "        Columns (list) are the categorical variables we want to check\n",
    "    \n",
    "    '''\n",
    "    print('FREQUENCIES')\n",
    "    print('===========')\n",
    "    \n",
    "    print('\\n')\n",
    "    #We check frequencies\n",
    "    \n",
    "    for var in columns:\n",
    "    \n",
    "        print(var)\n",
    "        print('=====')\n",
    "        print(data[var].value_counts())\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    print('CROSSTABS')\n",
    "    print('===========')\n",
    "    \n",
    "    #We check combinations\n",
    "    \n",
    "    combs = list(combinations(columns,2))\n",
    "    \n",
    "    for comb in combs:\n",
    "        print(comb[0]+' x '+comb[1])\n",
    "        print('=====')\n",
    "        print(pd.crosstab(data[comb[0]],data[comb[1]]))\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hesa_parser(url,out_name,skip=16,encoding='utf-8'):\n",
    "    '''\n",
    "    Function to obtain and parse data from the HESA website \n",
    "    \n",
    "    Args:\n",
    "        url (str) is the location of the csv file\n",
    "        out_name (str) is the saved name of the file\n",
    "        skip is the number of rows to skip (we could automate this by looking for rows at the top with lots of nans)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Request and parse\n",
    "    rs = requests.get(url)\n",
    "    \n",
    "    #Parse the file\n",
    "    parsed = rs.content.decode(encoding)\n",
    "    \n",
    "    #Save it\n",
    "    \n",
    "    with open(f'../../data/raw/hesa/{out_name}.txt','w') as outfile:\n",
    "        outfile.write(parsed)\n",
    "        \n",
    "    #Read it.\n",
    "    my_csv = pd.read_csv(f'../../data/raw/hesa/{out_name}.txt',skiprows=skip)\n",
    "    \n",
    "    #Clean column names\n",
    "    my_csv.columns = tidy_cols(my_csv)\n",
    "    \n",
    "    \n",
    "    return(my_csv)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gimme_nuts(lat,lon,level=2):\n",
    "    '''\n",
    "    Function to extract nuts information from a pair of coordinates\n",
    "    \n",
    "    Args:\n",
    "        lat (float) is the latitude\n",
    "        lon (float) is the longitude\n",
    "        level (int) is the NUTS level we want\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    info = nf.find(lat=lat,lon=lon)\n",
    "    try:\n",
    "        nuts_id = [x['NUTS_ID'] for x in info if x['LEVL_CODE']==level][0]\n",
    "        nuts_name = [x['NUTS_NAME'] for x in info if x['LEVL_CODE']==level][0]\n",
    "    #print(info)\n",
    "    \n",
    "    #nuts_id = info[level]['NUTS_ID']\n",
    "    #nuts_name = info[level]['NUTS_NAME']\n",
    "    \n",
    "    except:\n",
    "        print(f'failed with {np.round(lat,2)},{np.round(lon,2)}')\n",
    "        nuts_id = np.nan\n",
    "        nuts_name=np.nan\n",
    "    \n",
    "    return([nuts_id,nuts_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_data(df_1,df_2,id_1,id_2,name_1,name_2):\n",
    "    '''\n",
    "    We use this function to check if the ids in two datasets we are merging are consistent.\n",
    "    \n",
    "    Args:\n",
    "        dfs are the dfs we want to compare\n",
    "        ids are the ids we want to check\n",
    "        names are the names we want to use to explore the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print('In 1 but not in 2')\n",
    "    print('==================')\n",
    "    d1_miss = set(df_1[id_1].dropna())-set(df_2[id_2])\n",
    "    print(set(df_1.loc[[x in d1_miss for x in df_1[id_1]]][name_1]))\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    print('In 2 but not in 1')\n",
    "    print('==================')\n",
    "    d2_miss = set(df_2[id_2].dropna())-set(df_1[id_1])\n",
    "    print(set(df_2.loc[[x in d2_miss for x in df_2[id_2]]][name_2]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NUTS aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nuts_estimate(data,nuts_lookup,counter,name,year_var=None):\n",
    "    '''\n",
    "    This function takes hesa data and creates a nuts estimate\n",
    "    \n",
    "    Args:\n",
    "        data (df) where we have already selected variables of interest eg mode of employment\n",
    "        nuts (dict) is the ukprn - nuts name and code lookup\n",
    "        counter (str) is the variable with counts that we are interested in\n",
    "        year_var (str) is the variable containing the years we want to group by. If None, then we are not grouping by year\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    d = data.copy()\n",
    "    \n",
    "    #Add the nuts names and codes\n",
    "    d['nuts_name'],d['nuts_code'] = [[nuts_lookup[ukprn][var] if ukprn in nuts_lookup.keys() else np.nan for ukprn in data['ukprn']] for\n",
    "                                     var in ['nuts_name','nuts_code']]\n",
    "    \n",
    "    #We are focusing on numbers\n",
    "    d[counter] = d[counter].astype(float)\n",
    "    \n",
    "    #Group results by year?\n",
    "    if year_var == None:\n",
    "        out = d.groupby(['nuts_name','nuts_code'])[counter].sum()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        out = d.groupby(['nuts_name','nuts_code',year_var])[counter].sum()\n",
    "        \n",
    "    \n",
    "    out.name = name\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_nuts_estimates(data,nuts_lookup,variables,select_var,value,year_var=None):\n",
    "    '''\n",
    "    Creates NUTS estimates for multiple variables.\n",
    "    \n",
    "    Args:\n",
    "        data (df) is the filtered dataframe\n",
    "        select_var (str) is the variable we want to use to select values\n",
    "        nuts_lookup (dict) is the lookup between universities and nuts\n",
    "        variables (list) is the list of variables for which we want to generate the analysis\n",
    "        value (str) is the field that contains the numerical value we want to aggregate in the dataframe\n",
    "        year_var (str) is the year_variable. If none, then we are not interested in years\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if year_var==None:\n",
    "        concat = pd.concat([make_nuts_estimate(data.loc[data[select_var]==m],nuts_lookup,value,m) for m in \n",
    "                  variables],axis=1)\n",
    "    \n",
    "    #If we want to do this by year then we will create aggregates by nuts name and code and year and then concatenate over columns \n",
    "    else:\n",
    "        \n",
    "        year_store = []\n",
    "        \n",
    "        for m in variables:\n",
    "            \n",
    "            y = make_nuts_estimate(data.loc[data[select_var]==m],nuts_lookup,value,m,year_var='academic_year')\n",
    "            \n",
    "            year_store.append(y)\n",
    "            \n",
    "        concat = pd.concat(year_store,axis=1)\n",
    "                \n",
    "    return(concat)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_indicator(table,target_path,var_lookup,year_var,nuts_var='nuts_code',nuts_spec=2018):\n",
    "    '''\n",
    "    We use this function to create and save indicators using our standardised format.\n",
    "    \n",
    "    Args:\n",
    "        table (df) is a df with relevant information\n",
    "        target_path (str) is the location of the directory where we want to save the data (includes interim and processed)\n",
    "        var_lookup (dict) is a lookup to rename the variable into our standardised name\n",
    "        year (str) is the name of the year variable\n",
    "        nuts_var (str) is the name of the NUTS code variable. We assume it is nuts_code\n",
    "        nuts_spec (y) is the value of the NUTS specification. We assume we are working with 2018 NUTS\n",
    "    \n",
    "    '''\n",
    "    #Copy\n",
    "    t = table.reset_index(drop=False)\n",
    "    \n",
    "    #Reset index (we assume that the index is the nuts code, var name and year - this might need to be changed)\n",
    "    \n",
    "    \n",
    "    #Process the interim data into an indicator\n",
    "    \n",
    "    #This is the variable name and code\n",
    "    var_name = list(var_lookup.keys())[0]\n",
    "    \n",
    "    var_code = list(var_lookup.values())[0]\n",
    "    \n",
    "    #Focus on those\n",
    "    t = t[[year_var,nuts_var,var_name]]\n",
    "    \n",
    "    #Add the nuts specification\n",
    "    t['nuts_year_spec'] = nuts_spec\n",
    "    \n",
    "    #Rename variables\n",
    "    t.rename(columns={var_name:var_code,year_var:'year',nuts_var:'nuts_id'},inplace=True)\n",
    "\n",
    "    \n",
    "    #Reorder variables\n",
    "    t = t[['year','nuts_id','nuts_year_spec',var_code]]\n",
    "    \n",
    "    print(t.head())\n",
    "    \n",
    "    #Save in the processed folder\n",
    "    t.to_csv(f'../../data/processed/{target_path}/{var_code}.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hesa directory in raw and processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs('hesa',['raw','processed','interim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University metadata\n",
    "\n",
    "The [learning providers website](http://learning-provider.data.ac.uk/) contains information about universities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_meta = pd.read_csv('http://learning-provider.data.ac.uk/data/learning-providers-plus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_meta.columns = tidy_cols(uni_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This initialises an object to label lons and lats with their NUTS code\n",
    "nf = NutsFinder(scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dict from ukprn to name, nuts2\n",
    "uni_nuts = {row['ukprn']:{'name':row['view_name'],\n",
    "                          'nuts_code':gimme_nuts(lat=row['latitude'],lon=row['longitude'])[0],\n",
    "                          'nuts_name':gimme_nuts(lat=row['latitude'],lon=row['longitude'])[1]} for rid,row in uni_meta.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_staff = hesa_parser('https://www.hesa.ac.uk/data-and-analysis/staff/table-1.csv','staff',skip=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also downloaded staff qualifications but probably won't use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_staff = hesa_parser('https://www.hesa.ac.uk/data-and-analysis/staff/table-8.csv','qual_staff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = hesa_parser('https://www.hesa.ac.uk/data-and-analysis/estates/data.csv','spaces',11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem graduates\n",
    "\n",
    "This is a larger zip file so we have to use a different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Request\n",
    "rs = requests.get('https://www.hesa.ac.uk/data-and-analysis/students/table-13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip and save the file\n",
    "\n",
    "#Note that the file contains tables for various years. We keep all of them\n",
    "years = ['2014-15','2015-16','2016-17','2017-18','2018-19']\n",
    "\n",
    "out_files = [zipfile.ZipFile(io.BytesIO(rs.content)).extract(f'table-13-({year}).csv','../../data/raw/hesa/') for year in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use a pipe to assign a year to each df and concatenate into a single df\n",
    "# graduates_all_years = pd.concat(\n",
    "#     [pd.read_csv(out_files[n],skiprows=14).pipe(lambda x: x.assign(academic_year = year)) for n,year in enumerate(years)],axis=0)\n",
    "\n",
    "\n",
    "graduates_all_years = pd.concat(\n",
    "    [pd.read_csv(out_files[n],skiprows=14) for n in np.arange(len(out_files))],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduates_all_years.columns = tidy_cols(graduates_all_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduates_all_years.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing\n",
    "\n",
    "Processing involves:\n",
    "\n",
    "1. Select variables we want to use for the indicators (eg year, mode of study)\n",
    "2. Label the data with the NUTS information\n",
    "3. Group over NUTS and generate estimate\n",
    "\n",
    "We can probably create a function to do 2 and 3 taking the subset data as input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all the universities in HESA have metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_data(res_staff,uni_meta,'ukprn','ukprn','he_provider','view_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_data(spaces,uni_meta,'ukprn','ukprn','he_provider','view_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graduates takes too long to run!\n",
    "\n",
    "#We create a shorter version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_short = graduates_all_years.loc[(graduates_all_years['level_of_study']=='All')&((graduates_all_years['mode_of_study']=='Full-time'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_data(grad_short,uni_meta,'ukprn','ukprn','he_provider','view_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are small universities - I have checked names and found that the difference between sets isn't caused by mismatches in codes (eg the same university having different codes in different sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Number of research staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_staff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check categories in interesting columns\n",
    "interesting_cols = ['mode_of_employment','atypical_marker','contract_marker','academic_year','activity_standard_occupational_classification']\n",
    "\n",
    "#check_categories(res_staff,interesting_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_staff_filter = {'mode_of_employment':'All','contract_marker':'Academic',\n",
    "                   'activity_standard_occupational_classification':'Total academic staff',\n",
    "                   'country_of_he_provider':'All','region_of_he_provider':'All'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_filtered = filter_data(res_staff,res_staff_filter)\n",
    "\n",
    "len(res_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_academics = make_nuts_estimate(res_filtered,uni_nuts,'number','academic_staff','academic_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_academics.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Research space\n",
    "\n",
    "See some variable definitions for estates [here](https://www.hesa.ac.uk/support/definitions/estates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_interesting_cols = ['academic_year','country_of_he_provider','region_of_he_provider','category_marker','table']\n",
    "\n",
    "#check_categories(spaces,sp_interesting_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains a lot of information. We will only focus on a couple of variables:\n",
    "\n",
    "* Total number of buildings\n",
    "* Total site area\n",
    "* Research income\n",
    "* Research student FTE\n",
    "* Total site area (hectares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_vars = ['Research income (£)','Research student FTE','Total number of buildings','Total site area (hectares)']\n",
    "\n",
    "nuts_spaces = multiple_nuts_estimates(spaces,uni_nuts,space_vars,'category_marker','value',year_var='academic_year')\n",
    "\n",
    "nuts_spaces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Number of STEM graduates\n",
    "\n",
    "The graduates file is quite big so I focus on grad short, which considers all full time graduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_interesting_columns = ['level_of_study','mode_of_study','country_of_he_provider',\n",
    "                            'region_of_he_provider','subject_of_study_marker','subject_of_study']\n",
    "\n",
    "#check_categories(grad_short,grad_interesting_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_filter = {'country_of_he_provider':'All','region_of_he_provider':'All'}\n",
    "\n",
    "grad_filtered = filter_data(grad_short,grad_filter)\n",
    "\n",
    "grad_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disciplines = set(grad_filtered['subject_of_study'])\n",
    "\n",
    "nuts_disciplines = multiple_nuts_estimates(grad_filtered,uni_nuts,disciplines,'subject_of_study','number',year_var='academic_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_disciplines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of postgraduates\n",
    "\n",
    "This is a flavour of the variable above where we count the number of research postgraduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will filter the data to focus on full time postgraduate researchers\n",
    "post_grad_filter = grad_filter.copy()\n",
    "\n",
    "post_grad_filter['level_of_study'] = 'Postgraduate (research)'\n",
    "post_grad_filter['mode_of_study'] = 'Full-time'\n",
    "post_grad_filter['subject_of_study_marker']= 'Subject area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_grad_filtered = filter_data(graduates_all_years,post_grad_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_postgrads = make_nuts_estimate(post_grad_filtered,uni_nuts,'number','postgrad_research',year_var='academic_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_postgrads.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output indicators\n",
    "\n",
    "Produce output indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Research students (issue 90)\n",
    "\n",
    "This is simply the number of postgraduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the interim file\n",
    "nuts_postgrads.to_csv(f'../../data/interim/{today_str}_hesa_postgraduates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(nuts_spaces,'hesa',{'Research student FTE':'fte_research_students'},'academic_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or number of FTE research students? Perhaps focus on this one as it has been subject to less transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(nuts_spaces,'hesa',{'Research income (£)':'gbp_research_income'},'academic_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Students in STEM disciplines (issue 91)\n",
    "\n",
    "Load definition of STEM disciplines (which needs to be checked by BEIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed file\n",
    "\n",
    "nuts_disciplines.to_csv(f'../../data/interim/hesa/{today_str}_students_disciplines_nuts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/aux/stem_hesa.txt','r') as infile:\n",
    "    \n",
    "    stem_hesa = infile.read().split('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_students = nuts_disciplines[stem_hesa].sum(axis=1)\n",
    "\n",
    "stem_students.name = 'stem_students'\n",
    "\n",
    "stem_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(stem_students,'hesa',{'stem_students':'total_stem_students'},'academic_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Stem postgraduates (issue 112)\n",
    "\n",
    "We need to recalculate the STEM values focusing only on research postgraduates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_grad_filter = {'country_of_he_provider':'All','region_of_he_provider':'All','mode_of_study':'Full-time','level_of_study':'Postgraduate (research)',\n",
    "                   'subject_of_study_marker':'Subject area'}\n",
    "\n",
    "postgrad_filtered = filter_data(grad_short,post_grad_filter)\n",
    "\n",
    "post_grad_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed that the postgraduate data only seems to be available for subject areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract information\n",
    "nuts_postgrad_discipline = multiple_nuts_estimates(post_grad_filtered,uni_nuts,disciplines,'subject_of_study','number',year_var='academic_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract STEM subjects\n",
    "stem_postgrads_detailed = nuts_postgrad_discipline[stem_hesa]\n",
    "\n",
    "stem_postgrads_detailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate them\n",
    "stem_postgraduates = stem_postgrads_detailed.sum(axis=1)\n",
    "\n",
    "stem_postgraduates.name = 'stem_postgraduate_students'\n",
    "\n",
    "stem_postgraduates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(stem_postgraduates,'hesa',{'stem_postgraduate_students':'total_stem_postgraduates'},'academic_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Area of university states (Issue 56)\n",
    "\n",
    "These indicators will require little processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_spaces.to_csv(f'../../data/interim/hesa/{today_str}_university_spaces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(nuts_spaces,'hesa',{'Total site area (hectares)':'area_university_site'},'academic_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Number of buildings (issue 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(nuts_spaces,'hesa',{'Total number of buildings':'total_university_buildings'},'academic_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Research income (issue 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(nuts_spaces,'hesa',{'Research income (£)':'gbp_research_income'},'academic_year')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
