{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HESA \n",
    "\n",
    "Some code to collect HESA data and merge it with university metadata in order to create indicators about skills supply in the UK.\n",
    "\n",
    "We are interested in the following indicators:\n",
    "\n",
    "* Research staff: https://www.hesa.ac.uk/data-and-analysis/staff/working-in-he\n",
    "* Research spaces: https://www.hesa.ac.uk/data-and-analysis/estates/table-1\n",
    "* STEM graduates produced: https://www.hesa.ac.uk/data-and-analysis/students/what-study#\n",
    "* PhD graduates produced: this is also in https://www.hesa.ac.uk/data-and-analysis/students/what-study#\n",
    "\n",
    "\n",
    "See [this table](https://docs.google.com/spreadsheets/d/1V2fAQcvuLsoImwo6uLdyIK3x80pBNoX97CxsxkjvRP4/edit?usp=sharing) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from nuts_finder import NutsFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_cols(my_csv):\n",
    "    '''\n",
    "    Tidies column names ie lower and replace spaces with underscores\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return([re.sub(' ','_',col.lower()) for col in my_csv.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data,var_val_pairs):\n",
    "    '''\n",
    "    We use this to filter the data more easily than using pandas subsetting\n",
    "    \n",
    "    Args:\n",
    "        data (df) is a dataframe\n",
    "        var_val pairs (dict) is a dictionary where the keys are variables and the value are values\n",
    "\n",
    "    \n",
    "    '''\n",
    "    d = data.copy()\n",
    "    \n",
    "    for k,v in var_val_pairs.items():\n",
    "        d = d.loc[d[k]==v]\n",
    "        \n",
    "    return(d.reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hesa_parser(url,out_name,skip=16,encoding='utf-8'):\n",
    "    '''\n",
    "    Function to obtain and parse data from the HESA website \n",
    "    \n",
    "    Args:\n",
    "        url (str) is the location of the csv file\n",
    "        out_name (str) is the saved name of the file\n",
    "        skip is the number of rows to skip (we could automate this by looking for rows at the top with lots of nans)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Request and parse\n",
    "    rs = requests.get(url)\n",
    "    \n",
    "    #Parse the file\n",
    "    parsed = rs.content.decode(encoding)\n",
    "    \n",
    "    #Save it\n",
    "    \n",
    "    with open(f'../../data/raw/hesa/{out_name}.txt','w') as outfile:\n",
    "        outfile.write(parsed)\n",
    "        \n",
    "    #Read it.\n",
    "    my_csv = pd.read_csv(f'../../data/raw/hesa/{out_name}.txt',skiprows=skip)\n",
    "    \n",
    "    #Clean column names\n",
    "    my_csv.columns = tidy_cols(my_csv)\n",
    "    \n",
    "    \n",
    "    return(my_csv)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gimme_nuts(lat,lon,level=2):\n",
    "    '''\n",
    "    Function to extract nuts information from a pair of coordinates\n",
    "    \n",
    "    Args:\n",
    "        lat (float) is the latitude\n",
    "        lon (float) is the longitude\n",
    "        level (int) is the NUTS level we want\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    info = nf.find(lat=round(lat,5),lon=round(lon,5))\n",
    "    try:\n",
    "        nuts_id = [x['NUTS_ID'] for x in info if x['LEVL_CODE']==level][0]\n",
    "        nuts_name = [x['NUTS_NAME'] for x in info if x['LEVL_CODE']==level][0]\n",
    "    #print(info)\n",
    "    \n",
    "    #nuts_id = info[level]['NUTS_ID']\n",
    "    #nuts_name = info[level]['NUTS_NAME']\n",
    "    \n",
    "    except:\n",
    "        print(f'failed with {np.round(lat,5)},{np.round(lon,5)}')\n",
    "        nuts_id = np.nan\n",
    "        nuts_name=np.nan\n",
    "    \n",
    "    return([nuts_id,nuts_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_data(df_1,df_2,id_1,id_2,name_1,name_2):\n",
    "    '''\n",
    "    We use this function to check if the ids in two datasets we are merging are consistent.\n",
    "    \n",
    "    Args:\n",
    "        dfs are the dfs we want to compare\n",
    "        ids are the ids we want to check\n",
    "        names are the names we want to use to explore the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print('In 1 but not in 2')\n",
    "    print('==================')\n",
    "    d1_miss = set(df_1[id_1].dropna())-set(df_2[id_2])\n",
    "    print(set(df_1.loc[[x in d1_miss for x in df_1[id_1]]][name_1]))\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    print('In 2 but not in 1')\n",
    "    print('==================')\n",
    "    d2_miss = set(df_2[id_2].dropna())-set(df_1[id_1])\n",
    "    print(set(df_2.loc[[x in d2_miss for x in df_2[id_2]]][name_2]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NUTS aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nuts_estimate(data,nuts_lookup,counter,name):\n",
    "    '''\n",
    "    This function takes hesa data and creates a nuts estimate\n",
    "    \n",
    "    Args:\n",
    "        data (df) where we have already selected variables of interest eg mode of employment\n",
    "        nuts (dict) is the ukprn - nuts name and code lookup\n",
    "        counter (str) is the variable with counts that we are interested in\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    d = data.copy()\n",
    "    \n",
    "    #Add the nuts names and codes\n",
    "    d['nuts_name'],d['nuts_code'] = [[nuts_lookup[ukprn][var] if ukprn in nuts_lookup.keys() else np.nan for ukprn in data['ukprn']] for\n",
    "                                     var in ['nuts_name','nuts_code']]\n",
    "    \n",
    "    #We are focusing on numbers\n",
    "    d[counter] = d[counter].astype(float)\n",
    "    \n",
    "    out = d.groupby(['nuts_name','nuts_code'])[counter].sum()\n",
    "    \n",
    "    out.name = name\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_nuts_estimates(data,nuts_lookup,variables,select_var,value):\n",
    "    '''\n",
    "    Creates NUTS estimates for multiple variables.\n",
    "    \n",
    "    Args:\n",
    "        data (df) is the filtered dataframe\n",
    "        select_var (str) is the variable we want to use to select values\n",
    "        nuts_lookup (dict) is the lookup between universities and nuts\n",
    "        variables (list) is the list of variables for which we want to generate the analysis\n",
    "        value (str) is the field that contains the numerical value we want to aggregate in the dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    concat = pd.concat([make_nuts_estimate(data.loc[data[select_var]==m],nuts_lookup,value,m) for m in \n",
    "              variables],axis=1)\n",
    "    \n",
    "    return(concat)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hesa directory in raw and processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'hesa' not in os.listdir('../../data/raw'):\n",
    "    os.mkdir('../../data/raw/hesa')\n",
    "    \n",
    "if 'hesa' not in os.listdir('../../data/processed'):\n",
    "    os.mkdir('../../data/processed/hesa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University metadata\n",
    "\n",
    "The [learning providers website](http://learning-provider.data.ac.uk/) contains information about universities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_meta = pd.read_csv('http://learning-provider.data.ac.uk/data/learning-providers-plus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_meta.columns = tidy_cols(uni_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This initialises an object to label lons and lats with their NUTS code\n",
    "nf = NutsFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dict from ukprn to name, nuts2\n",
    "uni_nuts = {row['ukprn']:{'name':row['view_name'],\n",
    "                          'nuts_code':gimme_nuts(lat=row['latitude'],lon=row['longitude'])[0],\n",
    "                          'nuts_name':gimme_nuts(lat=row['latitude'],lon=row['longitude'])[1]} for rid,row in uni_meta.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_staff = hesa_parser('https://www.hesa.ac.uk/data-and-analysis/staff/table-1.csv','staff',skip=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also downloaded staff qualifications but probably won't use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_staff = hesa_parser('https://www.hesa.ac.uk/data-and-analysis/staff/table-8.csv','qual_staff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = hesa_parser('https://www.hesa.ac.uk/data-and-analysis/estates/data.csv','spaces',11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem graduates\n",
    "\n",
    "This is a larger zip file so we have to use a different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Request\n",
    "rs = requests.get('https://www.hesa.ac.uk/data-and-analysis/students/table-13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip and save the file\n",
    "out = zipfile.ZipFile(io.BytesIO(rs.content)).extract('table-13.csv','../../data/raw/hesa/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduates = pd.read_csv(out,skiprows=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduates.columns = tidy_cols(graduates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing\n",
    "\n",
    "Processing involves:\n",
    "\n",
    "1. Select variables we want to use in the analysis (eg year, mode of study)\n",
    "2. Label the data with the NUTS information\n",
    "3. Group over NUTS and generate estimate\n",
    "\n",
    "We can probably create a function to do 2 and 3 taking the subset data as input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all the universities in HESA have metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_data(res_staff,uni_meta,'ukprn','ukprn','he_provider','view_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_data(spaces,uni_meta,'ukprn','ukprn','he_provider','view_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graduates takes too long to run!\n",
    "\n",
    "#We create a shorter version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_short = graduates.loc[(graduates['level_of_study']=='All')&((graduates['mode_of_study']=='Full-time'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_data(grad_short,uni_meta,'ukprn','ukprn','he_provider','view_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are small universities - I have checked names and found that the difference between sets isn't caused by mismatches in codes (eg the same university having different codes in different sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Number of research staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_staff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categories(data,columns):\n",
    "    '''\n",
    "    This counts frequencies of categorical variables. We use it to decide what variables to choose, and to avoid double counting\n",
    "    \n",
    "    Args:\n",
    "        Data (df) is the data\n",
    "        Columns (list) are the categorical variables we want to check\n",
    "    \n",
    "    '''\n",
    "    print('FREQUENCIES')\n",
    "    print('===========')\n",
    "    \n",
    "    print('\\n')\n",
    "    #We check frequencies\n",
    "    \n",
    "    for var in columns:\n",
    "    \n",
    "        print(var)\n",
    "        print('=====')\n",
    "        print(data[var].value_counts())\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    print('CROSSTABS')\n",
    "    print('===========')\n",
    "    \n",
    "    #We check combinations\n",
    "    \n",
    "    combs = list(combinations(columns,2))\n",
    "    \n",
    "    for comb in combs:\n",
    "        print(comb[0]+' x '+comb[1])\n",
    "        print('=====')\n",
    "        print(pd.crosstab(data[comb[0]],data[comb[1]]))\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check categories in interesting columns\n",
    "interesting_cols = ['mode_of_employment','atypical_marker','contract_marker','academic_year','activity_standard_occupational_classification']\n",
    "\n",
    "#check_categories(res_staff,interesting_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_staff_filter = {'mode_of_employment':'All','contract_marker':'Academic','academic_year':'2017/18',\n",
    "                   'activity_standard_occupational_classification':'Total academic staff',\n",
    "                   'country_of_he_provider':'All','region_of_he_provider':'All'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_filtered = filter_data(res_staff,res_staff_filter)\n",
    "\n",
    "len(res_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_academics = make_nuts_estimate(res_filtered,uni_nuts,'number','academic_staff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_academics.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Research space\n",
    "\n",
    "See some variable definitions for estates [here](https://www.hesa.ac.uk/support/definitions/estates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_interesting_cols = ['academic_year','country_of_he_provider','region_of_he_provider','category_marker','table']\n",
    "\n",
    "#check_categories(spaces,sp_interesting_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains a lot of information. We will only focus on a couple of variables:\n",
    "\n",
    "* Total number of buildings\n",
    "* Total site area\n",
    "* Research income\n",
    "* Research student FTE\n",
    "* Total site area (hectares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We focus on the most recent year\n",
    "space_filter = {'academic_year':'2017/18'}\n",
    "\n",
    "space_filtered = filter_data(spaces,space_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_vars = ['Research income (Â£)','Research student FTE','Total number of buildings','Total site area (hectares)']\n",
    "\n",
    "nuts_spaces = multiple_nuts_estimates(space_filtered,uni_nuts,space_vars,'category_marker','value')\n",
    "\n",
    "nuts_spaces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Number of STEM graduates\n",
    "\n",
    "The graduates file is quite big so I focus on grad short, which considers all full time graduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_interesting_columns = ['level_of_study','mode_of_study','country_of_he_provider',\n",
    "                            'region_of_he_provider','subject_of_study_marker','subject_of_study']\n",
    "\n",
    "#check_categories(grad_short,grad_interesting_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_filter = {'country_of_he_provider':'All','region_of_he_provider':'All','academic_year':'2017/18'}\n",
    "\n",
    "grad_filtered = filter_data(grad_short,grad_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disciplines = set(grad_filtered['subject_of_study'])\n",
    "\n",
    "nuts_disciplines = multiple_nuts_estimates(grad_filtered,uni_nuts,disciplines,'subject_of_study','number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_disciplines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of postgraduates\n",
    "\n",
    "This is a flavour of the variable above where we count the number of research postgraduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will filter the data to focus on full time postgraduate researchers\n",
    "post_grad_filter = grad_filter.copy()\n",
    "\n",
    "post_grad_filter['level_of_study'] = 'Postgraduate (research)'\n",
    "post_grad_filter['mode_of_study'] = 'Full-time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_grad_filtered = filter_data(graduates,post_grad_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_postgrads = make_nuts_estimate(post_grad_filtered,uni_nuts,'number','postgrad_research')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_postgrads.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We ended with the following datasets for academic year 2017/18:\n",
    "\n",
    "* `nuts_academics` contains academic staff per NUTS2\n",
    "* `nuts_spaces` contains key scale variables per NUTS2. This includes:\n",
    "  * Research income ([Definition](https://www.hesa.ac.uk/collection/c17042/a/firei))\n",
    "  * Research student FTE ([definition](https://www.hesa.ac.uk/collection/c17042/a/stftere) - these are doctorates or masters)\n",
    "  * Total number of buildings\n",
    "  * Total site area ([definition](https://www.hesa.ac.uk/collection/c17042/a/sitearea))\n",
    "* `nuts_disciplines` contains student enrolment by discipline\n",
    "* `nuts_postgrads` contains postgraduates in research per discipline (Full time) This should be strongly correlated (if not the same) as Research student FTE in `nuts_spaces`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_disciplines[['(N7) Office skills','(A0) Broadly-based programmes within medicine and dentistry']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of disciplines with zero levels of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = pd.concat([nuts_academics,nuts_spaces,nuts_disciplines,nuts_postgrads],axis=1)\n",
    "\n",
    "all_vars.drop(axis=1,labels=['(N7) Office skills','(A0) Broadly-based programmes within medicine and dentistry'],inplace=True)\n",
    "\n",
    "all_vars.columns = tidy_cols(all_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could it be that they are never studied part time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "all_vars.to_csv(f'../../data/processed/hesa/{today_str}_hesa_data_nuts_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.clustermap(all_vars.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
