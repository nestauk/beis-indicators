{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HE-BCI\n",
    "\n",
    "Here we collect data from the Higher-Education Business Community Interaction Survey available from the HESA website ([link](https://www.hesa.ac.uk/data-and-analysis/business-community))\n",
    "\n",
    "The structure is very similar to other HESA we collected in `01_jmg` so eventually we might want to merge both notebooks. I will definitely be reusing a lot of the code here.\n",
    "\n",
    "In terms of indicators, we would like to create the following:\n",
    "\n",
    "* Graduate start-ups rate (HE-BCI)\n",
    "* Research resource (income) per spin-out (HE-BCI)\n",
    "* Average external investment per formal spin-out (HE-BCI)\n",
    "* Licensing and other IP income as proportion of research income (HE-BCI)\n",
    "* Contract research income with businesses (HE-BCI)\n",
    "* Consultancy income with businesses (HE-BCI)\n",
    "* Contract research income with the public and third sector (HE-BCI)\n",
    "* Consultancy income with the public and third sector  (HE-BCI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import zipfile\n",
    "import io\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from nuts_finder import NutsFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = str(datetime.datetime.today()).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_cols(my_csv):\n",
    "    '''\n",
    "    Tidies column names ie lower and replace spaces with underscores\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return([re.sub(' ','_',col.lower()) for col in my_csv.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data,var_val_pairs):\n",
    "    '''\n",
    "    We use this to filter the data more easily than using pandas subsetting\n",
    "    \n",
    "    Args:\n",
    "        data (df) is a dataframe\n",
    "        var_val pairs (dict) is a dictionary where the keys are variables and the value are values\n",
    "\n",
    "    \n",
    "    '''\n",
    "    d = data.copy()\n",
    "    \n",
    "    for k,v in var_val_pairs.items():\n",
    "        d = d.loc[d[k]==v]\n",
    "        \n",
    "    return(d.reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categories(data,columns):\n",
    "    '''\n",
    "    This counts frequencies of categorical variables. We use it to decide what variables to choose, and to avoid double counting\n",
    "    \n",
    "    Args:\n",
    "        Data (df) is the data\n",
    "        Columns (list) are the categorical variables we want to check\n",
    "    \n",
    "    '''\n",
    "    print('FREQUENCIES')\n",
    "    print('===========')\n",
    "    \n",
    "    print('\\n')\n",
    "    #We check frequencies\n",
    "    \n",
    "    for var in columns:\n",
    "    \n",
    "        print(var)\n",
    "        print('=====')\n",
    "        print(data[var].value_counts())\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    print('CROSSTABS')\n",
    "    print('===========')\n",
    "    \n",
    "    #We check combinations\n",
    "    \n",
    "    combs = list(combinations(columns,2))\n",
    "    \n",
    "    for comb in combs:\n",
    "        print(comb[0]+' x '+comb[1])\n",
    "        print('=====')\n",
    "        print(pd.crosstab(data[comb[0]],data[comb[1]]))\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hesa_parser(url,out_name,skip=16,encoding='utf-8'):\n",
    "    '''\n",
    "    Function to obtain and parse data from the HESA website \n",
    "    \n",
    "    Args:\n",
    "        url (str) is the location of the csv file\n",
    "        out_name (str) is the saved name of the file\n",
    "        skip is the number of rows to skip (we could automate this by looking for rows at the top with lots of nans)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Request and parse\n",
    "    rs = requests.get(url)\n",
    "    \n",
    "    #Parse the file\n",
    "    parsed = rs.content.decode(encoding)\n",
    "    \n",
    "    #Save it\n",
    "    \n",
    "    with open(f'../../data/raw/hesa/{out_name}.txt','w') as outfile:\n",
    "        outfile.write(parsed)\n",
    "        \n",
    "    #Read it.\n",
    "    my_csv = pd.read_csv(f'../../data/raw/hesa/{out_name}.txt',skiprows=skip)\n",
    "    \n",
    "    #Clean column names\n",
    "    my_csv.columns = tidy_cols(my_csv)\n",
    "    \n",
    "    \n",
    "    return(my_csv)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gimme_nuts(lat,lon,level=2):\n",
    "    '''\n",
    "    Function to extract nuts information from a pair of coordinates\n",
    "    \n",
    "    Args:\n",
    "        lat (float) is the latitude\n",
    "        lon (float) is the longitude\n",
    "        level (int) is the NUTS level we want\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    info = nf.find(lat=lat,lon=lon)\n",
    "    try:\n",
    "        nuts_id = [x['NUTS_ID'] for x in info if x['LEVL_CODE']==level][0]\n",
    "        nuts_name = [x['NUTS_NAME'] for x in info if x['LEVL_CODE']==level][0]\n",
    "    #print(info)\n",
    "    \n",
    "    #nuts_id = info[level]['NUTS_ID']\n",
    "    #nuts_name = info[level]['NUTS_NAME']\n",
    "    \n",
    "    except:\n",
    "        print(f'failed with {np.round(lat,2)},{np.round(lon,2)}')\n",
    "        nuts_id = np.nan\n",
    "        nuts_name=np.nan\n",
    "    \n",
    "    return([nuts_id,nuts_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_data(df_1,df_2,id_1,id_2,name_1,name_2):\n",
    "    '''\n",
    "    We use this function to check if the ids in two datasets we are merging are consistent.\n",
    "    \n",
    "    Args:\n",
    "        dfs are the dfs we want to compare\n",
    "        ids are the ids we want to check\n",
    "        names are the names we want to use to explore the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print('In 1 but not in 2')\n",
    "    print('==================')\n",
    "    d1_miss = set(df_1[id_1].dropna())-set(df_2[id_2])\n",
    "    print(set(df_1.loc[[x in d1_miss for x in df_1[id_1]]][name_1]))\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    print('In 2 but not in 1')\n",
    "    print('==================')\n",
    "    d2_miss = set(df_2[id_2].dropna())-set(df_1[id_1])\n",
    "    print(set(df_2.loc[[x in d2_miss for x in df_2[id_2]]][name_2]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NUTS aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nuts_estimate(data,nuts_lookup,counter,name,year_var='academic_year'):\n",
    "    '''\n",
    "    This function takes hesa data and creates a nuts estimate\n",
    "    \n",
    "    Args:\n",
    "        data (df) where we have already selected variables of interest eg mode of employment\n",
    "        nuts (dict) is the ukprn - nuts name and code lookup\n",
    "        counter (str) is the variable with counts that we are interested in\n",
    "        year_var (str) is the variable containing the years we want to group by. If None, then we are not grouping by year\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    d = data.copy()\n",
    "    \n",
    "    #Add the nuts names and codes\n",
    "    d['nuts_name'],d['nuts_code'] = [[nuts_lookup[ukprn][var] if ukprn in nuts_lookup.keys() else np.nan for ukprn in data['ukprn']] for\n",
    "                                     var in ['nuts_name','nuts_code']]\n",
    "    \n",
    "    #We are focusing on numbers\n",
    "    d[counter] = d[counter].astype(float)\n",
    "    \n",
    "    #Group results by year?\n",
    "    if year_var == None:\n",
    "        out = d.groupby(['nuts_name','nuts_code'])[counter].sum()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        out = d.groupby(['nuts_name','nuts_code',year_var])[counter].sum()\n",
    "        \n",
    "    \n",
    "    out.name = name\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_nuts_estimates(data,nuts_lookup,variables,select_var,value,year_var='academic_year'):\n",
    "    '''\n",
    "    Creates NUTS estimates for multiple variables.\n",
    "    \n",
    "    Args:\n",
    "        data (df) is the filtered dataframe\n",
    "        select_var (str) is the variable we want to use to select values\n",
    "        nuts_lookup (dict) is the lookup between universities and nuts\n",
    "        variables (list) is the list of variables for which we want to generate the analysis\n",
    "        value (str) is the field that contains the numerical value we want to aggregate in the dataframe\n",
    "        year_var (str) is the year_variable. If none, then we are not interested in years\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if year_var==None:\n",
    "        concat = pd.concat([make_nuts_estimate(data.loc[data[select_var]==m],nuts_lookup,value,m) for m in \n",
    "                  variables],axis=1)\n",
    "    \n",
    "    #If we want to do this by year then we will create aggregates by nuts name and code and year and then concatenate over columns \n",
    "    else:\n",
    "        \n",
    "        year_store = []\n",
    "        \n",
    "        for m in variables:\n",
    "            \n",
    "            y = make_nuts_estimate(data.loc[data[select_var]==m],nuts_lookup,value,m,year_var='academic_year')\n",
    "            \n",
    "            year_store.append(y)\n",
    "            \n",
    "        concat = pd.concat(year_store,axis=1)\n",
    "                \n",
    "    return(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_academic_year(df,year_var = 'academic_year',position=0):\n",
    "    '''\n",
    "    This function converts an academic year variable from HESA into a year (int)\n",
    "    \n",
    "    Args:\n",
    "        df (df) with the academic year we want to convert\n",
    "        year_var (str) is the name of the year variable\n",
    "        position (int) is the position of the year. We default to 0 (first year)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Make copy\n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Reset index so we can work with it easily\n",
    "    df_2 = df_2.reset_index(level=2)\n",
    "    \n",
    "    #Create the new year variable by splitting the academic year variable on /\n",
    "    df_2[year_var] = [int(x.split('/')[position]) if position==0 else int('20'+x.split('/')[position])  for x in df_2[year_var]]\n",
    "    \n",
    "    #Reappend the year index\n",
    "    df_2.set_index(year_var,append=True,inplace=True)\n",
    "    \n",
    "    return(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_indicator(table,target_path,var_lookup,year_var,nuts_var='nuts_code',nuts_spec=2018,decimals=3):\n",
    "    '''\n",
    "    We use this function to create and save indicators using our standardised format.\n",
    "    \n",
    "    Args:\n",
    "        table (df) is a df with relevant information\n",
    "        target_path (str) is the location of the directory where we want to save the data (includes interim and processed)\n",
    "        var_lookup (dict) is a lookup to rename the variable into our standardised name\n",
    "        year (str) is the name of the year variable\n",
    "        nuts_var (str) is the name of the NUTS code variable. We assume it is nuts_code\n",
    "        nuts_spec (y) is the value of the NUTS specification. We assume we are working with 2018 NUTS\n",
    "    \n",
    "    '''\n",
    "    #Copy\n",
    "    t = table.reset_index(drop=False)\n",
    "    \n",
    "    #Reset index (we assume that the index is the nuts code, var name and year - this might need to be changed)\n",
    "    \n",
    "    \n",
    "    #Process the interim data into an indicator\n",
    "    \n",
    "    #This is the variable name and code\n",
    "    var_name = list(var_lookup.keys())[0]\n",
    "    \n",
    "    var_code = list(var_lookup.values())[0]\n",
    "    \n",
    "    #Focus on those\n",
    "    t = t[[year_var,nuts_var,var_name]]\n",
    "    \n",
    "    #Add the nuts specification\n",
    "    t['nuts_year_spec'] = nuts_spec\n",
    "    \n",
    "    #Rename variables\n",
    "    t.rename(columns={var_name:var_code,year_var:'year',nuts_var:'nuts_id'},inplace=True)\n",
    "\n",
    "    #Round variables\n",
    "    t[var_code] = [np.round(x,decimals) if decimals>0 else int(x) for x in t[var_code]]\n",
    "    \n",
    "    \n",
    "    #Reorder variables\n",
    "    t = t[['year','nuts_id','nuts_year_spec',var_code]]\n",
    "    \n",
    "    print(t.head())\n",
    "    \n",
    "    #Save in the processed folder\n",
    "    t.to_csv(f'../../data/processed/{target_path}/{var_code}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hesa directory in raw and processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'hebci' not in os.listdir('../../data/raw'):\n",
    "    os.mkdir('../../data/raw/hebci')\n",
    "    \n",
    "if 'hebci' not in os.listdir('../../data/interim'):\n",
    "    os.mkdir('../../data/interim/hebci')\n",
    "    \n",
    "if 'hebci' not in os.listdir('../../data/processed'):\n",
    "    os.mkdir('../../data/processed/hebci')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University metadata\n",
    "\n",
    "The [learning providers website](http://learning-provider.data.ac.uk/) contains information about universities. \n",
    "\n",
    "We have geocoded them in `0-jmg-university...`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/metadata/uni_nuts.txt','r') as infile:\n",
    "    \n",
    "    uni_nuts = literal_eval(infile.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin-out activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-4e.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin = hesa_parser(url_1,'spin',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licensing income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-4d.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = hesa_parser(url_2,'ip',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Services income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_3 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-2a.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = hesa_parser(url_3,'services',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative research involving public funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_4 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab = hesa_parser(url_4,'collab',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spinout related\n",
    "\n",
    "Here we will focus on the number of spinouts in different categories and the levels of external investment that they have received.\n",
    "\n",
    "This includes issues `77`, `78`, `79`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns_spin = ['country_of_he_provider','region_of_he_provider','academic_year','metric','category_marker']\n",
    "\n",
    "#check_categories(spin,interesting_columns_spin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a df with the number of active firms per category. This can be subset later\n",
    "\n",
    "#Create a dict to filter the data\n",
    "spin_n_filter = {'metric':'Number of active firms'}\n",
    "\n",
    "spins_number= multiple_nuts_estimates(filter_data(spin,spin_n_filter),uni_nuts,set(spin['category_marker']),'category_marker','value',year_var='academic_year')\n",
    "\n",
    "spins_number.columns = [x+'_n' for x in tidy_cols(spins_number)]\n",
    "\n",
    "spins_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spins_number.sort_values('graduate_start-ups_n',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one focused on levels of investment received\n",
    "\n",
    "#Create a dict to filter the data\n",
    "spin_inv_filter = {'academic_year':'2017/18','metric':'Estimated external investment received (£ thousands)'}\n",
    "\n",
    "spins_inv= multiple_nuts_estimates(filter_data(spin,spin_inv_filter),uni_nuts,set(spin['category_marker']),'category_marker','value')\n",
    "\n",
    "spins_inv.columns = [x+'_inv_thGPB' for x in tidy_cols(spins_inv)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spins_inv.sort_values('staff_start-ups_inv_thGPB',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin_nuts = pd.concat([spins_number,spins_inv],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licensing income related\n",
    "\n",
    "We will extract total IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns_income = ['country_of_he_provider','region_of_he_provider','academic_year','category_marker','unit']\n",
    "\n",
    "#check_categories(income,interesting_columns_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_filter = {'academic_year':'2017/18','category_marker':'Total IP revenues'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_nuts = make_nuts_estimate(filter_data(income,income_filter),uni_nuts,'value','total_ip_revenues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_nuts.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### services related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns = ['type_of_service','type_of_organisation','number/value_marker']\n",
    "\n",
    "check_categories(services,interesting_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_filter = {'academic_year':'2017/18','type_of_organisation':'Total','number/value_marker':'Value'}\n",
    "\n",
    "services_nuts = multiple_nuts_estimates(filter_data(\n",
    "    services,services_filter),uni_nuts,set(services['type_of_service']),'type_of_service','number/value')\n",
    "\n",
    "services_nuts.columns = [x+'_thGBP' for x in tidy_cols(services_nuts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research collab related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns_collab = ['academic_year','source_of_public_funding','type_of_income']\n",
    "\n",
    "check_categories(collab,interesting_columns_collab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these looks particularly relevant / new compared to information we have collected from other HESA sources - we will leave them out for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hebci_nuts = pd.concat([spin_nuts,income_nuts,services_nuts],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.clustermap(hebci_nuts.apply(lambda x: zscore(x)).corr(),cmap='Oranges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hebci_nuts.to_csv(f'../../data/processed/hesa/{today_str}_hebci_nuts.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
