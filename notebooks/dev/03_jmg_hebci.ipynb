{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HE-BCI\n",
    "\n",
    "Here we collect data from the Higher-Education Business Community Interaction Survey available from the HESA website ([link](https://www.hesa.ac.uk/data-and-analysis/business-community))\n",
    "\n",
    "The structure is very similar to other HESA we collected in `01_jmg` so eventually we might want to merge both notebooks. I will definitely be reusing a lot of the code here.\n",
    "\n",
    "In terms of indicators, we would like to create the following:\n",
    "\n",
    "* Graduate start-ups rate (HE-BCI)\n",
    "* Research resource (income) per spin-out (HE-BCI)\n",
    "* Average external investment per formal spin-out (HE-BCI)\n",
    "* Licensing and other IP income as proportion of research income (HE-BCI)\n",
    "* Contract research income with businesses (HE-BCI)\n",
    "* Consultancy income with businesses (HE-BCI)\n",
    "* Contract research income with the public and third sector (HE-BCI)\n",
    "* Consultancy income with the public and third sector  (HE-BCI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import zipfile\n",
    "import io\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from nuts_finder import NutsFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = str(datetime.datetime.today()).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_cols(my_csv):\n",
    "    '''\n",
    "    Tidies column names ie lower and replace spaces with underscores\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return([re.sub(' ','_',col.lower()) for col in my_csv.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data,var_val_pairs):\n",
    "    '''\n",
    "    We use this to filter the data more easily than using pandas subsetting\n",
    "    \n",
    "    Args:\n",
    "        data (df) is a dataframe\n",
    "        var_val pairs (dict) is a dictionary where the keys are variables and the value are values\n",
    "\n",
    "    \n",
    "    '''\n",
    "    d = data.copy()\n",
    "    \n",
    "    for k,v in var_val_pairs.items():\n",
    "        d = d.loc[d[k]==v]\n",
    "        \n",
    "    return(d.reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categories(data,columns):\n",
    "    '''\n",
    "    This counts frequencies of categorical variables. We use it to decide what variables to choose, and to avoid double counting\n",
    "    \n",
    "    Args:\n",
    "        Data (df) is the data\n",
    "        Columns (list) are the categorical variables we want to check\n",
    "    \n",
    "    '''\n",
    "    print('FREQUENCIES')\n",
    "    print('===========')\n",
    "    \n",
    "    print('\\n')\n",
    "    #We check frequencies\n",
    "    \n",
    "    for var in columns:\n",
    "    \n",
    "        print(var)\n",
    "        print('=====')\n",
    "        print(data[var].value_counts())\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    print('CROSSTABS')\n",
    "    print('===========')\n",
    "    \n",
    "    #We check combinations\n",
    "    \n",
    "    combs = list(combinations(columns,2))\n",
    "    \n",
    "    for comb in combs:\n",
    "        print(comb[0]+' x '+comb[1])\n",
    "        print('=====')\n",
    "        print(pd.crosstab(data[comb[0]],data[comb[1]]))\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hesa_parser(url,out_name,skip=16,encoding='utf-8'):\n",
    "    '''\n",
    "    Function to obtain and parse data from the HESA website \n",
    "    \n",
    "    Args:\n",
    "        url (str) is the location of the csv file\n",
    "        out_name (str) is the saved name of the file\n",
    "        skip is the number of rows to skip (we could automate this by looking for rows at the top with lots of nans)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Request and parse\n",
    "    rs = requests.get(url)\n",
    "    \n",
    "    #Parse the file\n",
    "    parsed = rs.content.decode(encoding)\n",
    "    \n",
    "    #Save it\n",
    "    \n",
    "    with open(f'../../data/raw/hesa/{out_name}.txt','w') as outfile:\n",
    "        outfile.write(parsed)\n",
    "        \n",
    "    #Read it.\n",
    "    my_csv = pd.read_csv(f'../../data/raw/hesa/{out_name}.txt',skiprows=skip)\n",
    "    \n",
    "    #Clean column names\n",
    "    my_csv.columns = tidy_cols(my_csv)\n",
    "    \n",
    "    \n",
    "    return(my_csv)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gimme_nuts(lat,lon,level=2):\n",
    "    '''\n",
    "    Function to extract nuts information from a pair of coordinates\n",
    "    \n",
    "    Args:\n",
    "        lat (float) is the latitude\n",
    "        lon (float) is the longitude\n",
    "        level (int) is the NUTS level we want\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    info = nf.find(lat=lat,lon=lon)\n",
    "    try:\n",
    "        nuts_id = [x['NUTS_ID'] for x in info if x['LEVL_CODE']==level][0]\n",
    "        nuts_name = [x['NUTS_NAME'] for x in info if x['LEVL_CODE']==level][0]\n",
    "    #print(info)\n",
    "    \n",
    "    #nuts_id = info[level]['NUTS_ID']\n",
    "    #nuts_name = info[level]['NUTS_NAME']\n",
    "    \n",
    "    except:\n",
    "        print(f'failed with {np.round(lat,2)},{np.round(lon,2)}')\n",
    "        nuts_id = np.nan\n",
    "        nuts_name=np.nan\n",
    "    \n",
    "    return([nuts_id,nuts_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_data(df_1,df_2,id_1,id_2,name_1,name_2):\n",
    "    '''\n",
    "    We use this function to check if the ids in two datasets we are merging are consistent.\n",
    "    \n",
    "    Args:\n",
    "        dfs are the dfs we want to compare\n",
    "        ids are the ids we want to check\n",
    "        names are the names we want to use to explore the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print('In 1 but not in 2')\n",
    "    print('==================')\n",
    "    d1_miss = set(df_1[id_1].dropna())-set(df_2[id_2])\n",
    "    print(set(df_1.loc[[x in d1_miss for x in df_1[id_1]]][name_1]))\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    print('In 2 but not in 1')\n",
    "    print('==================')\n",
    "    d2_miss = set(df_2[id_2].dropna())-set(df_1[id_1])\n",
    "    print(set(df_2.loc[[x in d2_miss for x in df_2[id_2]]][name_2]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NUTS aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nuts_estimate(data,nuts_lookup,counter,name,year_var='academic_year'):\n",
    "    '''\n",
    "    This function takes hesa data and creates a nuts estimate\n",
    "    \n",
    "    Args:\n",
    "        data (df) where we have already selected variables of interest eg mode of employment\n",
    "        nuts (dict) is the ukprn - nuts name and code lookup\n",
    "        counter (str) is the variable with counts that we are interested in\n",
    "        year_var (str) is the variable containing the years we want to group by. If None, then we are not grouping by year\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    d = data.copy()\n",
    "    \n",
    "    #Add the nuts names and codes\n",
    "    d['nuts_name'],d['nuts_code'] = [[nuts_lookup[ukprn][var] if ukprn in nuts_lookup.keys() else np.nan for ukprn in data['ukprn']] for\n",
    "                                     var in ['nuts_name','nuts_code']]\n",
    "    \n",
    "    #We are focusing on numbers\n",
    "    d[counter] = d[counter].astype(float)\n",
    "    \n",
    "    #Group results by year?\n",
    "    if year_var == None:\n",
    "        out = d.groupby(['nuts_name','nuts_code'])[counter].sum()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        out = d.groupby(['nuts_name','nuts_code',year_var])[counter].sum()\n",
    "        \n",
    "    \n",
    "    out.name = name\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_nuts_estimates(data,nuts_lookup,variables,select_var,value,year_var='academic_year'):\n",
    "    '''\n",
    "    Creates NUTS estimates for multiple variables.\n",
    "    \n",
    "    Args:\n",
    "        data (df) is the filtered dataframe\n",
    "        select_var (str) is the variable we want to use to select values\n",
    "        nuts_lookup (dict) is the lookup between universities and nuts\n",
    "        variables (list) is the list of variables for which we want to generate the analysis\n",
    "        value (str) is the field that contains the numerical value we want to aggregate in the dataframe\n",
    "        year_var (str) is the year_variable. If none, then we are not interested in years\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if year_var==None:\n",
    "        concat = pd.concat([make_nuts_estimate(data.loc[data[select_var]==m],nuts_lookup,value,m) for m in \n",
    "                  variables],axis=1)\n",
    "    \n",
    "    #If we want to do this by year then we will create aggregates by nuts name and code and year and then concatenate over columns \n",
    "    else:\n",
    "        \n",
    "        year_store = []\n",
    "        \n",
    "        for m in variables:\n",
    "            \n",
    "            y = make_nuts_estimate(data.loc[data[select_var]==m],nuts_lookup,value,m,year_var='academic_year')\n",
    "            \n",
    "            year_store.append(y)\n",
    "            \n",
    "        concat = pd.concat(year_store,axis=1)\n",
    "                \n",
    "    return(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_academic_year(df,year_var = 'academic_year',position=0):\n",
    "    '''\n",
    "    This function converts an academic year variable from HESA into a year (int)\n",
    "    \n",
    "    Args:\n",
    "        df (df) with the academic year we want to convert\n",
    "        year_var (str) is the name of the year variable\n",
    "        position (int) is the position of the year. We default to 0 (first year)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Make copy\n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Reset index so we can work with it easily\n",
    "    df_2 = df_2.reset_index(level=2)\n",
    "    \n",
    "    #Create the new year variable by splitting the academic year variable on /\n",
    "    df_2[year_var] = [int(x.split('/')[position]) if position==0 else int('20'+x.split('/')[position])  for x in df_2[year_var]]\n",
    "    \n",
    "    #Reappend the year index\n",
    "    df_2.set_index(year_var,append=True,inplace=True)\n",
    "    \n",
    "    return(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_indicator(table,target_path,var_lookup,year_var,nuts_var='nuts_code',nuts_spec=2018,decimals=3):\n",
    "    '''\n",
    "    We use this function to create and save indicators using our standardised format.\n",
    "    \n",
    "    Args:\n",
    "        table (df) is a df with relevant information\n",
    "        target_path (str) is the location of the directory where we want to save the data (includes interim and processed)\n",
    "        var_lookup (dict) is a lookup to rename the variable into our standardised name\n",
    "        year (str) is the name of the year variable\n",
    "        nuts_var (str) is the name of the NUTS code variable. We assume it is nuts_code\n",
    "        nuts_spec (y) is the value of the NUTS specification. We assume we are working with 2018 NUTS\n",
    "    \n",
    "    '''\n",
    "    #Copy\n",
    "    t = table.reset_index(drop=False)\n",
    "    \n",
    "    #Reset index (we assume that the index is the nuts code, var name and year - this might need to be changed)\n",
    "    \n",
    "    \n",
    "    #Process the interim data into an indicator\n",
    "    \n",
    "    #This is the variable name and code\n",
    "    var_name = list(var_lookup.keys())[0]\n",
    "    \n",
    "    var_code = list(var_lookup.values())[0]\n",
    "    \n",
    "    #Focus on those\n",
    "    t = t[[year_var,nuts_var,var_name]]\n",
    "    \n",
    "    #Add the nuts specification\n",
    "    t['nuts_year_spec'] = nuts_spec\n",
    "    \n",
    "    #Rename variables\n",
    "    t.rename(columns={var_name:var_code,year_var:'year',nuts_var:'nuts_id'},inplace=True)\n",
    "\n",
    "    #Round variables\n",
    "    t[var_code] = [np.round(x,decimals) if decimals>0 else int(x) for x in t[var_code]]\n",
    "    \n",
    "    \n",
    "    #Reorder variables\n",
    "    t = t[['year','nuts_id','nuts_year_spec',var_code]]\n",
    "    \n",
    "    print(t.head())\n",
    "    \n",
    "    #Save in the processed folder\n",
    "    t.to_csv(f'../../data/processed/{target_path}/{var_code}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hesa directory in raw and processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'hebci' not in os.listdir('../../data/raw'):\n",
    "    os.mkdir('../../data/raw/hebci')\n",
    "    \n",
    "if 'hebci' not in os.listdir('../../data/interim'):\n",
    "    os.mkdir('../../data/interim/hebci')\n",
    "    \n",
    "if 'hebci' not in os.listdir('../../data/processed'):\n",
    "    os.mkdir('../../data/processed/hebci')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University metadata\n",
    "\n",
    "The [learning providers website](http://learning-provider.data.ac.uk/) contains information about universities. \n",
    "\n",
    "We have geocoded them in `0-jmg-university...`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/metadata/uni_nuts.txt','r') as infile:\n",
    "    \n",
    "    uni_nuts = literal_eval(infile.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin-out activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-4e.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin = hesa_parser(url_1,'spin',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licensing income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-4d.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = hesa_parser(url_2,'ip',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Services income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_3 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-2a.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = hesa_parser(url_3,'services',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative research involving public funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_4 = 'https://www.hesa.ac.uk/data-and-analysis/providers/business-community/table-1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab = hesa_parser(url_4,'collab',skip=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perf(table,perf,norm=False,sp_def='all',value='currency'):\n",
    "    '''\n",
    "    Function that calculates performance (employment, turnover, investment, active firms...) \n",
    "    \n",
    "    Args:\n",
    "        table (df) long table with the performance and spinoff category information\n",
    "        perf (str) measure of performance\n",
    "        sp_def (str) definition of spinoff\n",
    "        norm (str) if we want to normalise by the number of entities in the category\n",
    "        value (str) if currency multiply by 1000 to extract gpbs\n",
    "    \n",
    "    Returns a clean indicator\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    t = table.copy()\n",
    "    \n",
    "    #First get the financials\n",
    "    #Create a dict to filter the data\n",
    "    p_filter = {'metric':perf}\n",
    "    \n",
    "    #Extract the estimates\n",
    "    t_filt= multiple_nuts_estimates(filter_data(t,p_filter),uni_nuts,set(spin['category_marker']),'category_marker','value')\n",
    "    \n",
    "    #Are we subsetting by a category?\n",
    "    if sp_def == 'all':\n",
    "        t_filt = t_filt.sum(axis=1)\n",
    "    \n",
    "    else:\n",
    "        t_filt = t_filt[sp_def]\n",
    "    \n",
    "    #Tidy columns\n",
    "    t_filt.name = sp_def\n",
    "\n",
    "    #Scale if the value is a currency\n",
    "    if value=='currency':\n",
    "        t_filt = t_filt*1000\n",
    "        t_filt.name = 'gbp_'+t_filt.name\n",
    "    \n",
    "    #Do the same with the totals\n",
    "    \n",
    "    if norm == True:\n",
    "        \n",
    "        unit_filter = {'metric':'Number of active firms'}\n",
    "        \n",
    "        u_filt= multiple_nuts_estimates(filter_data(t,unit_filter),uni_nuts,set(spin['category_marker']),'category_marker','value')\n",
    "        \n",
    "        #Are we subsetting by a category?\n",
    "        if sp_def == 'all':\n",
    "            u_filt = u_filt.sum(axis=1)\n",
    "\n",
    "        else:\n",
    "            u_filt = u_filt[sp_def]\n",
    "\n",
    "        #Tidy columns\n",
    "        u_filt.name = 'all_comps'\n",
    "        \n",
    "        comb = pd.concat([t_filt,u_filt],axis=1)\n",
    "        \n",
    "        comb[f'{t_filt.name}_by_company']= comb[t_filt.name]/comb['all_comps']\n",
    "        \n",
    "        #Zeroes are nans (this is to avoid division by zero)\n",
    "        comb.fillna(0,inplace=True)\n",
    "        \n",
    "        return(comb)\n",
    "    \n",
    "    else:\n",
    "        return(t_filt)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. spinout related\n",
    "\n",
    "Here we will focus on the number of spinouts in different categories and the levels of external investment that they have received.\n",
    "\n",
    "This includes issues `77`, `78`, `79`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns_spin = ['country_of_he_provider','region_of_he_provider','academic_year','metric','category_marker']\n",
    "\n",
    "#check_categories(spin,interesting_columns_spin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin['metric'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin['category_marker'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graduate startup rate (item 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startup_rate = calculate_perf(spin,'Number of active firms',sp_def='Graduate start-ups',value='count')\n",
    "\n",
    "\n",
    "make_indicator(convert_academic_year(startup_rate),'hebci',{'Graduate start-ups':'total_active_graduate_startups'},year_var='academic_year',decimals=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turnover per spinout (item 78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_per_startup = calculate_perf(spin,'Estimated current turnover of all active firms (£ thousands)',norm=True,\n",
    "                                  sp_def='all',value='currency')\n",
    "\n",
    "make_indicator(convert_academic_year(turn_per_startup),'hebci',{'gbp_all_by_company':'gbp_turnover_per_active_spinoff'},year_var='academic_year',decimals=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average external investment per 'formal' (?) spinout (item 79)\n",
    "\n",
    "This is the same as above but with investment instead of turnover. We will focus on all companies because we have found some mistakes in the data - for example, Cranfield university has recorded £500K of investment recorded vs formal spinoffs, but no active companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_2 = spin.loc[\n",
    "#     (spin['category_marker']=='Formal spin-offs, not HEP owned')&(spin['academic_year']=='2014/15')].groupby(\n",
    "#     ['he_provider','metric'])['value'].sum().reset_index(drop=False).pivot(index='he_provider',columns='metric',values='value')\n",
    "\n",
    "# test_3 = test_2.loc[test_2['Number of active firms']==0]\n",
    "\n",
    "# test_3.sort_values('Estimated external investment received (£ thousands)')\n",
    "\n",
    "# #test_3.loc[test_3['Estimated current turnover of all active firms (£ thousands)']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_per_formal = calculate_perf(spin,'Estimated external investment received (£ thousands)',norm=True,\n",
    "                                  sp_def='all',value='currency')\n",
    "\n",
    "make_indicator(convert_academic_year(inv_per_formal),'hebci',{'gbp_all_by_company':'gbp_investment_per_active_spinoff'},year_var='academic_year',decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Licensing income related\n",
    "\n",
    "We will extract total IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns_income = ['country_of_he_provider','region_of_he_provider','academic_year','category_marker','unit']\n",
    "\n",
    "#check_categories(income,interesting_columns_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_filter = {'category_marker':'Total IP revenues'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that we are multiplying by 1000 to convert into GBP\n",
    "income_nuts = 1000*make_nuts_estimate(filter_data(ip,income_filter),uni_nuts,'value','total_ip_revenues',year_var='academic_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(convert_academic_year(income_nuts),'hebci',{'total_ip_revenues':'gbp_ip_revenues'},year_var='academic_year',decimals=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that they want this normalised by research income. We have already produced that indicator before. We need to pull the HESA data in to produce it. We combine them below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### services related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting_columns = ['type_of_service','type_of_organisation','number/value_marker']\n",
    "\n",
    "# check_categories(services,interesting_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_filter = {'type_of_service':'Consultancy','number/value_marker':'Value'}\n",
    "\n",
    "#Note that, as before, I am multiplying by 1000 as I am dealing with businesses\n",
    "services_nuts = 1000*multiple_nuts_estimates(filter_data(\n",
    "    services,services_filter),uni_nuts,set(services['type_of_organisation']),'type_of_organisation','number/value','academic_year')\n",
    "\n",
    "services_nuts.columns = tidy_cols(services_nuts)\n",
    "\n",
    "services_nuts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consultancy with business (#82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_nuts['business_consultancy'] = services_nuts.iloc[:,0]+services_nuts.iloc[:,2]\n",
    "\n",
    "make_indicator(convert_academic_year(services_nuts),'hebci',{'business_consultancy':'gbp_business_consulting'},year_var='academic_year',decimals=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consultancy with public sector organisations (#85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(convert_academic_year(services_nuts),'hebci',{'non-commercial_organisations':'gbp_non_business_consulting'},\n",
    "               year_var='academic_year',decimals=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contract research with business (#81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_res_filter = {'type_of_service':'Contract research','number/value_marker':'Value'}\n",
    "\n",
    "#Note that, as before, I am multiplying by 1000 as I am dealing with businesses\n",
    "res_nuts = 1000*multiple_nuts_estimates(filter_data(\n",
    "    services,contract_res_filter),uni_nuts,set(services['type_of_organisation']),'type_of_organisation','number/value','academic_year')\n",
    "\n",
    "res_nuts.columns = tidy_cols(res_nuts)\n",
    "\n",
    "res_nuts.head()\n",
    "\n",
    "#Add SME and non-SME contract research\n",
    "res_nuts['business_contract_research'] = res_nuts.iloc[:,0]+services_nuts.iloc[:,2]\n",
    "\n",
    "make_indicator(convert_academic_year(res_nuts),'hebci',{'business_contract_research':'gbp_business_contract_research'},year_var='academic_year',decimals=0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
