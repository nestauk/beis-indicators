{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gateway to Research\n",
    "\n",
    "This repo processes and produces indicators based on the Gateway to Research data.\n",
    "\n",
    "See [here](https://github.com/nestauk/gtr_data_processing) for a fuller description of data processing and enrichment.\n",
    "\n",
    "We will use a dataset with information about projects to create the following indicators:\n",
    "\n",
    "* Level of activity in and funding received by discipline (focusing on projects led by organisations in a location)\n",
    "* Number of participations in research (to capture research participation by organisations that aren't in NUTS with lots of universities)\n",
    "* Dyadic collaborations (instances where projects include pairs of organisations from the same location)\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "In this version of the notebook we are using `data_getters_lab` to get a processed version of the gtr data from Nesta DAPS. In a future version we will make the raw data available in a AWS bucket or something along the lines that non-Nesta researchers can use to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions etc down here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name,dirs = ['raw','processed']):\n",
    "    '''\n",
    "    Utility that creates directories to save the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for d in dirs:\n",
    "        if name not in os.listdir(f'../../data/{d}'):\n",
    "            os.mkdir(f'../../data/{d}/{name}')\n",
    "            \n",
    "def flat_freq(a_list):\n",
    "    '''\n",
    "    Return value counts for categories in a nested list\n",
    "    \n",
    "    '''\n",
    "    return(pd.Series([x for el in a_list for x in el]).value_counts())\n",
    "\n",
    "        \n",
    "\n",
    "def flatten_list(a_list):\n",
    "    \n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gtr_data(df,vars_to_parse):\n",
    "    '''\n",
    "    \n",
    "    This function parses strings into lists\n",
    "    \n",
    "    Args:\n",
    "        df is the df whose columns we want to parsr\n",
    "        vars_to_parse is a list with the variables to parse\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #If the column is in the list above, then parse it\n",
    "    for c in df.columns:\n",
    "    \n",
    "        if c in vars_to_parse:\n",
    "            df[c] = [literal_eval(x) for x in df[c]]\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hopefully this will allow us to convert into the new variables\n",
    "\n",
    "def convert_to_nut(list_of_lads,lookup,value):\n",
    "    '''\n",
    "    This function converts a list where every element is a lad code into a list where every element is a nuts code (or name!)\n",
    "    \n",
    "    Arguments:\n",
    "        list_of_lads (iterable) is an iterable where every element is a list of LAD codes.\n",
    "        lookups (dict) is a lookup between lad codes and NUT 2 codes and names\n",
    "        value (str) is whether we want to output NUT codes or names\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Note that we have some control flow to deal with LADS missing from the lookup (it might happen) \n",
    "    out = [[lookup[x][value] if x in lookup.keys() else np.nan for x in el] for el in list_of_lads]\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def convert_to_nut_multiple(df,var):\n",
    "    '''\n",
    "    \n",
    "    This function automates some of the above eg. we can choose one variable suffix (lead, all) and it automatically converts to nuts names and lads\n",
    "    \n",
    "    Note - this directly transforms the input df\n",
    "    \n",
    "    Arguments:\n",
    "        df (df) is the dataframe where we want to add the converted variables\n",
    "        var (str) is the list of lad codes that we want to convert to nuts codes and names\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    df[f'{var}_nut_code'],df[f'{var}_nut_name'] = [convert_to_nut(df[f'{var}_lad_code'],lads_to_nuts_lookup,n) for n in nut_vars]\n",
    "    \n",
    "    #return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_geo_var_stats(df,geo='nut',var='disc_top_discipline'):\n",
    "    '''\n",
    "    This function takes a df with project activity and creates discipline project counts and total amounts by geography\n",
    "    \n",
    "    Arguments:\n",
    "        df (df) is a dataframe where one of the columns is the geography and another the top discipline (or funder - we could change this)\n",
    "        geo (str) is the variable we want to use in the geo analysis\n",
    "        var (str) is the variable we want to get project counts and amounts of funding for\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "\n",
    "    \n",
    "    #Extract variable names and codes from list\n",
    "    df_2[f'lead_{geo}_name'],df_2[f'lead_{geo}_code']= [[x[0] if len(x)>0 else np.nan for x in df_2[var]] for var in [f'lead_{geo}_name',\n",
    "                                                                                        f'lead_{geo}_code']]\n",
    "    \n",
    "    \n",
    "    #Project frequencies by variable and geography\n",
    "    project_geo_counts = df_2.groupby([f'lead_{geo}_name',f'lead_{geo}_code','year'])[var].value_counts()\n",
    "\n",
    "    project_geo_counts.name = 'project_count'\n",
    "\n",
    "    #Pivot to create a wide version\n",
    "\n",
    "    project_wide = project_geo_counts.reset_index(drop=False).pivot_table(index=[f'lead_{geo}_name',f'lead_{geo}_code','year'],\n",
    "                                                                columns=var,values='project_count',aggfunc='sum').fillna(0)\n",
    "\n",
    "    project_wide.columns = [x+'_project_n' for x in project_wide]\n",
    "    \n",
    "    #Project funding by discipline\n",
    "    project_geo_funding = df_2.groupby([f'lead_{geo}_name',f'lead_{geo}_code',var,'year'])['amount'].sum()\n",
    "\n",
    "    fund_wide = project_geo_funding.reset_index(drop=False).pivot_table(index=[f'lead_{geo}_name',f'lead_{geo}_code','year'],\n",
    "                                                                columns=var,values='amount',aggfunc='sum').fillna(0)\n",
    "\n",
    "    fund_wide.columns = [x+'_funding_gpb' for x in fund_wide]\n",
    "    \n",
    "    out = pd.concat([project_wide,fund_wide],axis=1)\n",
    "    \n",
    "    return(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../utilities.py\n",
    "# Some utilities\n",
    "\n",
    "def make_data_dict(table,name,path,sample=5):\n",
    "    '''\n",
    "    A function to output the form for a data dictionary\n",
    "    \n",
    "    Args:\n",
    "        -table (df) is the df we want to create the data dictionary for\n",
    "        -name (str) of the df\n",
    "        -path (str) is the place where we want to save the file\n",
    "        \n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    types = [estimate_type(table[x],sample=sample) for x in table.columns]\n",
    "        \n",
    "    data_dict = pd.DataFrame()\n",
    "    data_dict['variable'] = table.columns\n",
    "        \n",
    "    data_dict['type'] = types\n",
    "    \n",
    "    data_dict['description'] = ['' for x in data_dict['variable']]\n",
    "        \n",
    "    out = os.path.join(path,f'{today_str}_{name}.csv')\n",
    "    \n",
    "    #print(data_dict.columns)\n",
    "    \n",
    "    data_dict.to_csv(out)\n",
    "    \n",
    "\n",
    "def estimate_type(variable,sample):\n",
    "    '''\n",
    "    Estimates the type of a column. \n",
    "\n",
    "    Args:\n",
    "        variable (iterable) with values\n",
    "        sample (n) is the number of values to test\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selection = random.sample(list(variable),sample)\n",
    "    \n",
    "    types = pd.Series([type(x) for x in selection]).value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    return(types.index[0])\n",
    "\n",
    "                           \n",
    "                           \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differently from other sources, we put the data in external because it has already been pre-processed\n",
    "make_dirs('gtr',['external','processed','interim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_df = pd.read_csv('http://geoportal1-ons.opendata.arcgis.com/datasets/9b4c94e915c844adb11e15a4b1e1294d_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a lookup between LADS and NUTS codes and names\n",
    "lads_to_nuts_lookup = {\n",
    "    rid: {'NUTS218CD':row['NUTS218CD'],'NUTS218NM':row['NUTS218NM']} for rid,row in nuts_df.set_index('LAD18CD')[['NUTS218CD','NUTS218NM']].iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also a nuts code to name lookup\n",
    "nuts_code_to_name = {x['NUTS218CD']:x['NUTS218NM'] for rid,x in nuts_df.drop_duplicates('NUTS218CD').iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect data\n",
    "\n",
    "From Nesta data getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_getters.labs.core import download_file\n",
    "\n",
    "\n",
    "def get_gtr(file,file_path,progress=True):\n",
    "    \"\"\" Fetch Gateway To Research predicted industries\n",
    "\n",
    "    Repo: https://github.com/nestauk/gtr_data_processing\n",
    "    Commit: cd3cddb\n",
    "    File: https://github.com/nestauk/gtr_data_processing/blob/master/notebooks/05_jmg_data_demo.ipynb\n",
    "\n",
    "    Args:\n",
    "        file_path (`str`, optional): Path to download to. If None, stream file.\n",
    "        progress (`bool`, optional): If `True` and `file_path` is not `None`,\n",
    "            display download progress.\n",
    "    \"\"\"\n",
    "    \n",
    "    return download_file(file_to_fetch=file, download_path=file_path+file, progress=progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the data and save in the folders we created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_org = get_gtr(file='17_9_2019_gtr_orgs.csv',file_path='../../data/external/gtr/',progress=False)\n",
    "\n",
    "gtr_proj = get_gtr(file='17_9_2019_gtr_projects.csv',file_path='../../data/external/gtr/',progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "gtr_proj = pd.read_csv('../../data/external/gtr/17_9_2019_gtr_projects.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some tidying up\n",
    "\n",
    "#Remove the unnamed columns\n",
    "gtr_proj = gtr_proj[[x for x in gtr_proj.columns if 'Unnamed' not in x]]\n",
    "\n",
    "#Parse lists\n",
    "list_var = [x for x in gtr_proj.columns if '_lad_' in x]\n",
    "\n",
    "gtr_proj = parse_gtr_data(gtr_proj,list_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not interested in columns between 24 and 102, which includes modelled industry and SDG variables\n",
    "\n",
    "gtr_proj = gtr_proj.iloc[:,[n for n in np.arange(0,len(gtr_proj.columns)) if n not in set(np.arange(25,103))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geoprocessing\n",
    "\n",
    "In previous work we geocoded the GtR data with LADS. Now we want to transfer this to NUTS2, the geographical unit of analysis for this project.\n",
    "\n",
    "Let's do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the functions defined above to convert lads to nuts\n",
    "nut_vars = ['NUTS218CD','NUTS218NM']\n",
    "\n",
    "\n",
    "convert_to_nut_multiple(gtr_proj,'lead')\n",
    "convert_to_nut_multiple(gtr_proj,'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_proj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discipline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each projects has a modelled discipline based on a predictive analysis that is fully reported in the source repo.\n",
    "\n",
    "#We assign each project to its top discipline\n",
    "\n",
    "disc_vars = [x for x in gtr_proj.columns if 'disc_' in x]\n",
    "\n",
    "gtr_proj['disc_top_discipline'] = gtr_proj[disc_vars].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create NUTS aggregations\n",
    "\n",
    "We are going to create the following:\n",
    "\n",
    "* Number of projects and level of funding led in the NUTS in various disciplines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discipline aggregates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the function above to create the geo by discipline aggregates\n",
    "disc = make_geo_var_stats(gtr_proj)\n",
    "\n",
    "#disc.to_csv(f'../../data/interim/gtr/{today_str}_nuts_discipline_activity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final processing and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/aux/gtr_stem_disciplines.txt','r') as infile:\n",
    "    \n",
    "    stem = infile.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of STEM projects per year and NUTS2 area\n",
    "disc_aggregate = disc[stem].sum(axis=1).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some final prep before saving\n",
    "#Rename columns\n",
    "disc_aggregate.rename(columns={'lead_nut_code':'nuts_id',0:'total_gtr_projects_stem'},inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_aggregate.drop('lead_nut_name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(disc_aggregate['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_aggregate.to_csv('../../data/processed/gtr/total_gtr_projects_stem.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluded indicators\n",
    "\n",
    "**These are not included in the final inventory so we have excluded them from the analysis (for now)**\n",
    "\n",
    "* Number of participations in research\n",
    "* Number of local collaboration in research (how many projects contain the same NUT more than once?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Volume of participation of research**\n",
    "\n",
    "We also want to capture the level of participation from organisations in research even when they are not leading projects (ie sites of universities).\n",
    "\n",
    "We will simply count instances when an organisation appears in a project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_process(nuts_freqs,lookup,name):\n",
    "    '''\n",
    "    \n",
    "    This is to avoid repetition when doing the final processing of nuts frequency series\n",
    "    \n",
    "    Args:\n",
    "        nut_freqs (series) a nuts freq where the index are nuts codes\n",
    "    \n",
    "    '''\n",
    "    #Reset the index\n",
    "    expand = nuts_freqs.reset_index(drop=False)\n",
    "    \n",
    "    #Add names using the lookup\n",
    "    expand['nuts_name'] = expand['index'].map(lookup)\n",
    "    \n",
    "    #Rename columns\n",
    "    expand.rename(columns={'index':'nuts_code',0:name},inplace=True)\n",
    "    \n",
    "    out = expand.set_index(['nuts_name','nuts_code'])\n",
    "    \n",
    "    return(out)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many times does a NUTS 2 area participate at least once in a project\n",
    "\n",
    "research_participation = final_process(\n",
    "    flat_freq([list(set(x)) for x in gtr_proj['all_nut_code']]),nuts_code_to_name,'proj_participation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local collaborations**\n",
    "\n",
    "Finally, we want to calculate how many projects involve local collaborations.\n",
    "\n",
    "This one is a bit more complicated. We will do the following:\n",
    "\n",
    "* Extract pairs of combinations from each project NUTS list and concatenate them (this is an edge list)\n",
    "* Set them. If len ==1 then that is a local collaboration\n",
    "* Remove all len >1 & count them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nice list comprehension: create a list of pairwise combinations from the collaborations, and keep those whose set length is 1 (both NUTS are the same)\n",
    "edge_list = [list(collab) for collab in [set(x) for x in flatten_list([list(combinations(x,2)) for x in gtr_proj['all_nut_code']])] if len(collab)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_collabs = final_process(flat_freq(edge_list),nuts_code_to_name,'local_collaborations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_activity = pd.concat([research_participation,local_collabs],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_activity.to_csv(f'../../data/interim/gtr/{today_str}_research_act_collab.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick plot comparing project participation vs local collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_activity.sort_values('local_collaborations').apply(lambda x: x/x.sum()).plot.barh(figsize=(8,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "West London is massively overrepresented in the local collaborations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
