{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrunchBase indicators: number of organisations and investment.\n",
    "\n",
    "Here we produce indicators about level of Venture & Seed Funding in the UK using proprietary CrunchBase data licensed by Nesta. \n",
    "\n",
    "This involves:\n",
    "\n",
    "* Download the data from Nesta DAPS system\n",
    "* Merge organisations & funders to create org - funding matches\n",
    "* Geocode with NUTS2 and LEPS geographies\n",
    "* Create indicators\n",
    "  * This will be based on a function that subsets by year and distinguishes between seed funding and venture capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import csv\n",
    "from data_getters.labs.core import download_file\n",
    "from ast import literal_eval\n",
    "from data_getters.core import get_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirs\n",
    "\n",
    "if 'crunchbase' not in os.listdir('../../data/raw'):\n",
    "    os.makedirs('../../data/raw/crunchbase')\n",
    "\n",
    "\n",
    "if 'crunchbase' not in os.listdir('../../data/interim'):\n",
    "    os.makedirs('../../data/interim/crunchbase')\n",
    "\n",
    "    \n",
    "if 'crunchbase' not in os.listdir('../../data/processed/'):\n",
    "    os.makedirs('../../data/processed/crunchbase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../utilities.py\n",
    "# Some utilities\n",
    "\n",
    "import random\n",
    "\n",
    "def make_data_dict(table,name,path,sample=5):\n",
    "    '''\n",
    "    A function to output the form for a data dictionary\n",
    "    \n",
    "    Args:\n",
    "        -table (df) is the df we want to create the data dictionary for\n",
    "        -name (str) of the df\n",
    "        -path (str) is the place where we want to save the file\n",
    "        \n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    types = [estimate_type(table[x],sample=sample) for x in table.columns]\n",
    "        \n",
    "    data_dict = pd.DataFrame()\n",
    "    data_dict['variable'] = table.columns\n",
    "        \n",
    "    data_dict['type'] = types\n",
    "    \n",
    "    data_dict['description'] = ['' for x in data_dict['variable']]\n",
    "        \n",
    "    out = os.path.join(path,f'{today_str}_{name}.csv')\n",
    "    \n",
    "    #print(data_dict.columns)\n",
    "    \n",
    "    data_dict.to_csv(out)\n",
    "    \n",
    "\n",
    "def estimate_type(variable,sample):\n",
    "    '''\n",
    "    Estimates the type of a column. \n",
    "\n",
    "    Args:\n",
    "        variable (iterable) with values\n",
    "        sample (n) is the number of values to test\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selection = random.sample(list(variable),sample)\n",
    "    \n",
    "    types = pd.Series([type(x) for x in selection]).value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    return(types.index[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df,name,path,today=today_str):\n",
    "    '''\n",
    "    Utility to save processed data quicker\n",
    "    \n",
    "    Arguments:\n",
    "        df (df) is the dataframe we want to save\n",
    "        name (str) is the name of the file\n",
    "        path (str) is the path where we want to save the file\n",
    "        today (str) is the day when the data is saved\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df.to_csv(f'{path}/{today_str}_{name}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daps_data(table,connection,chunksize=1000):\n",
    "    '''\n",
    "    Utility function to get data from DAPS with less faff\n",
    "    \n",
    "    Args:\n",
    "        -table is the SQL table in DAPS that we are extracting\n",
    "        -connection is the database connection we are using\n",
    "        -Chunksize are the chunks to download\n",
    "    \n",
    "    Returns:\n",
    "        -A dataframe with the data we have collected\n",
    "    \n",
    "    '''\n",
    "    #Get chunks\n",
    "    chunks = pd.read_sql_table(table, connection, chunksize=chunksize)\n",
    "    \n",
    "    #Create df\n",
    "    df = pd.concat(chunks)\n",
    "    \n",
    "    #Return data\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conversion(x,tid):\n",
    "    '''\n",
    "    Function to convert funding rounds from CrunchBase into GBP\n",
    "    \n",
    "    Args:\n",
    "        transaction: a transaction from the CB funding rounds dataset\n",
    "        tid: transaction id (to track issues)\n",
    "    \n",
    "    Returns:\n",
    "        A conversion (if possible)\n",
    "    '''\n",
    "    \n",
    "    #If an amount is not in GBP convert to GBP, if not, keep it as is\n",
    "    \n",
    "    #The currency converter doesn't work with Lebanese pounds so we will skip that\n",
    "    if (x['raised_amount_currency_code']=='LBP')|(x['raised_amount_currency_code']==None):\n",
    "        return(np.nan)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        try:\n",
    "            out = x['raised_amount']*c.get_rate(\n",
    "                x['raised_amount_currency_code'],'GBP',x['announced_on']) if x['raised_amount_currency_code']!='GBP' else x['raised_amount']\n",
    "            return(out)\n",
    "\n",
    "        except: \n",
    "            print(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_investments(df,geography):\n",
    "    '''\n",
    "    This function aggregates level of funding over a geography and investment type for a selected period\n",
    "    \n",
    "    Arguments:\n",
    "        df: df with investment levels by geocoded organisaton, year and type\n",
    "        years: (list) year range to be considered\n",
    "        geography: (str) what geography name to use\n",
    "    \n",
    "    Returns a table where the rows are the geography and the columns are levels of funding by investment type\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Period to consider\n",
    "    #period = np.arange(years[0],years[1])\n",
    "    \n",
    "    #Subset by the year\n",
    "    #df_in_period = df.loc[[x.year in period for x in df['announced_on']]]\n",
    "    \n",
    "    #Pivot\n",
    "    out = pd.pivot_table(df,index=[geography,'announced_year'],columns='investment_type',values='raised_amount_gbp',aggfunc='sum').fillna(0)\n",
    "    \n",
    "    return(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_indicator(table,target_path,var_lookup,year_var,nuts_var='nuts_code',nuts_spec=2018,decimals=3):\n",
    "    '''\n",
    "    We use this function to create and save indicators using our standardised format.\n",
    "    \n",
    "    Args:\n",
    "        table (df) is a df with relevant information\n",
    "        target_path (str) is the location of the directory where we want to save the data (includes interim and processed)\n",
    "        var_lookup (dict) is a lookup to rename the variable into our standardised name\n",
    "        year (str) is the name of the year variable\n",
    "        nuts_var (str) is the name of the NUTS code variable. We assume it is nuts_code\n",
    "        nuts_spec (y) is the value of the NUTS specification. We assume we are working with 2018 NUTS\n",
    "    \n",
    "    '''\n",
    "    #Copy\n",
    "    t = table.reset_index(drop=False)\n",
    "    \n",
    "    #Reset index (we assume that the index is the nuts code, var name and year - this might need to be changed)\n",
    "    \n",
    "    \n",
    "    #Process the interim data into an indicator\n",
    "    \n",
    "    #This is the variable name and code\n",
    "    var_name = list(var_lookup.keys())[0]\n",
    "    \n",
    "    var_code = list(var_lookup.values())[0]\n",
    "    \n",
    "    #Focus on those\n",
    "    t = t[[year_var,nuts_var,var_name]]\n",
    "    \n",
    "    #Add the nuts specification\n",
    "    t['nuts_year_spec'] = nuts_spec\n",
    "    \n",
    "    #Rename variables\n",
    "    t.rename(columns={var_name:var_code,year_var:'year',nuts_var:'nuts_id'},inplace=True)\n",
    "\n",
    "    #Round variables\n",
    "    t[var_code] = [np.round(x,decimals) if decimals>0 else int(x) for x in t[var_code]]\n",
    "    \n",
    "    \n",
    "    #Reorder variables\n",
    "    t = t[['year','nuts_id','nuts_year_spec',var_code]]\n",
    "    \n",
    "    print(t.head())\n",
    "    \n",
    "    #Save in the processed folder\n",
    "    t.to_csv(f'../../data/processed/{target_path}/{var_code}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CrunchBase data using DAPS\n",
    "\n",
    "my_config = '../../mysqldb_team.config'\n",
    "\n",
    "#Create connection with SQL\n",
    "con = get_engine(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organisations\n",
    "\n",
    "This is the list of organisations we want to wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "cb_orgs = get_daps_data('crunchbase_organizations',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_orgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every organisation has an id and a location id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funding rounds\n",
    "\n",
    "Funding rounds for organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_funding_rounds = get_daps_data('crunchbase_funding_rounds',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_funding_rounds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each funding round has the company name and location id, the investment type and the year. This means that we don't need the organisation data for the funding measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse geocoded place ids\n",
    "\n",
    "We have reverse geocoded place ids with their NUTS and LEPS code in notebook `0_rev_geocoder`. \n",
    "\n",
    "We load that information here and use it to generate indicators of activity by NUTS and LEPS area in the UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pd.read_csv('../../data/interim/crunchbase/2020_02_18_rev_geocoded_places.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Number of technology companies indicator\n",
    "\n",
    "This is the number of active companies in a NUTS or LEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_orgs_geo = pd.merge(cb_orgs,places,left_on='location_id',right_on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_orgs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus on active companies\n",
    "\n",
    "uk_comps = cb_orgs_geo.loc[(cb_orgs_geo['primary_role']=='company')&(cb_orgs_geo['status']=='operating')&(cb_orgs_geo['country']=='United Kingdom')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_comps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_comps['nuts218nm'].value_counts().head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_comps['lep17nm'].value_counts().head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create activity files\n",
    "nuts_orgs,leps_orgs = [uk_comps.groupby(var).size() for var in [['nuts218nm','nuts218cd'],['lep17nm','lep17cd']]]\n",
    "\n",
    "#Name the series\n",
    "nuts_orgs.name='company_n'\n",
    "leps_orgs.name='company_n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Level of VC and seed funding indicator\n",
    "\n",
    "Here we merge the geocoded df with the funding one and then create a function that aggregates funding by location & category for a threshold year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_geo = pd.merge(cb_funding_rounds,places,left_on='location_id',right_on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will need a python currency converter.\n",
    "inv_geo['raised_amount_currency_code'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_geo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion strategy\n",
    "\n",
    "We will use the announcement date and the currency information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "c = CurrencyRates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If an amount is not in GBP convert to GBP, if not, keep it as is\n",
    "\n",
    "inv_geo['raised_amount_gbp'] = [make_conversion(x,rid) for rid,x in inv_geo.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_geo['announced_year'] = [x.year for x in inv_geo['announced_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the indicators_w_threshold function to calculate levels of investment by NUTS and LEPS area\n",
    "inv_nuts_2,inv_leps = [aggregate_investments(inv_geo,var) for var in ['nuts218cd','lep17cd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_nuts_2_recent = inv_nuts_2.loc[inv_nuts_2.index.get_level_values('announced_year')>2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inv_nuts_2.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_venture = inv_nuts_2_recent[[x for x in inv_nuts_2.columns if 'series_' in x]].sum(axis=1)\n",
    "\n",
    "inv_venture.name = 'venture_capital_investment'\n",
    "\n",
    "inv_venture = pd.DataFrame(inv_venture)\n",
    "\n",
    "inv_venture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Org_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../../data/processed/crunchbase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file,name in zip([nuts_orgs,inv_nuts_2,leps_orgs,inv_leps],['nuts_2_orgs','nuts_2_investment','leps_orgs','leps_investment']):\n",
    "#     save_data(file,name,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(inv_venture,'crunchbase',{'venture_capital_investment':'gbp_venture_capital_received'},\n",
    "              year_var='announced_year',nuts_spec=2016,nuts_var='nuts218cd',decimals=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
