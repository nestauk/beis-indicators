{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse geocoding\n",
    "\n",
    "This is the prototype for a simple tool to bin entities into a geography based on their lats and lons.\n",
    "\n",
    "In addition to prototyping the binner and testing it with the places data from CrunchBase, we will use this script to download the shapefiles for NUTS2 and LEPS that we will be using in the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "from data_getters.core import get_engine\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO, BytesIO\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daps_data(table,connection,chunksize=1000):\n",
    "    '''\n",
    "    Utility function to get data from DAPS with less faff\n",
    "    \n",
    "    Args:\n",
    "        -table is the SQL table in DAPS that we are extracting\n",
    "        -connection is the database connection we are using\n",
    "        -Chunksize are the chunks to download\n",
    "    \n",
    "    Returns:\n",
    "        -A dataframe with the data we have collected\n",
    "    \n",
    "    '''\n",
    "    #Get chunks\n",
    "    chunks = pd.read_sql_table(table, connection, chunksize=chunksize)\n",
    "    \n",
    "    #Create df\n",
    "    df = pd.concat(chunks)\n",
    "    \n",
    "    #Return data\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapes(url,file_name,path='../../data/shapefiles/'):\n",
    "    '''\n",
    "    Utility function to extract and save a bunch of shapefiles from the ONS open geography portal\n",
    "    \n",
    "    Arguments:\n",
    "        url: url for the shapefile zip\n",
    "        file_name: name of the file where we want to extract the data\n",
    "    \n",
    "    '''\n",
    "    #Get the data\n",
    "    print(f'getting {file_name}...')\n",
    "    req = requests.get(url)\n",
    "    \n",
    "    #Parse the content\n",
    "    z = ZipFile(BytesIO(req.content))\n",
    "    \n",
    "    #Save\n",
    "    print(f'saving {file_name}...')\n",
    "    z.extractall(f'{path}{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CrunchBase data using DAPS\n",
    "\n",
    "my_config = '../../mysqldb_team.config'\n",
    "\n",
    "#Create connection with SQL\n",
    "con = get_engine(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df = get_daps_data('geographic_data',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes\n",
    "\n",
    "Download shapefiles from the [Open Geography Portal](https://geoportal.statistics.gov.uk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_url = 'https://opendata.arcgis.com/datasets/48b6b85bb7ea43699ee85f4ecd12fd36_1.zip?outSR=%7B%22latestWkid%22%3A27700%2C%22wkid%22%3A27700%7D'\n",
    "\n",
    "leps_url = 'https://opendata.arcgis.com/datasets/d4d519d1d1a1455a9b82331228f77489_1.zip?outSR=%7B%22latestWkid%22%3A27700%2C%22wkid%22%3A27700%7D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url,name in zip([nuts_url,leps_url],['nuts_2_2018','leps_2017']):\n",
    "    get_shapes(url,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write reverse geo_coder\n",
    "\n",
    "The reverse geocoder does a point of polygon merge between a df and a boundary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_geocoder(place_df,shape_path,place_id,\n",
    "                     coord_names= ['longitude','latitude']):\n",
    "    '''\n",
    "    The reverse geocoder takes a df with geographical coordinates and does a spatial merge with a shapefile.\n",
    "    \n",
    "    Args:\n",
    "        place_df (df). A dataframe where every row is an entity we want to reverse geocode\n",
    "        shape_path (str): The path for the shapefile (note we will need to project this to WGS84)\n",
    "        place_id (str): the name of the variable with the place id in place dfs\n",
    "        coord_names (list): Names for the lon and lat variables in the place_df\n",
    "        \n",
    "    Returns:\n",
    "        A spatially merged df with the location ids and their \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Read the shapefile    \n",
    "    print('Reading shapefile...')\n",
    "    \n",
    "    shape = gp.read_file(shape_path)\n",
    "    \n",
    "    #Change its projection so it can deal with lats and lons\n",
    "    shape = shape.to_crs({'init':'epsg:4326'})\n",
    "    \n",
    "    #Create a place_holder df (ho ho) where the index is the place id\n",
    "    place_holder = gp.GeoDataFrame(index=place_df[place_id],crs={'init':'epsg:4326'})\n",
    "    \n",
    "    #Create the geo field for spatial merge using Point\n",
    "    place_holder['geometry'] = [Point(x[coord_names[0]],x[coord_names[1]]) for rid,x in place_df.iterrows()]\n",
    "    \n",
    "    print('Joining...')\n",
    "    \n",
    "    #Spatial join: looks for points inside the polygons\n",
    "    joined = gp.sjoin(place_holder,shape,op='within')\n",
    "    \n",
    "    #Return the joined df\n",
    "    return(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We loop this function over different shapefiles\n",
    "nuts_path = '../../data/shapefiles/nuts_2_2018/NUTS_Level_2_January_2018_Full_Extent_Boundaries_in_the_United_Kingdom.shp'\n",
    "leps_path = '../../data/shapefiles/leps_2017/Local_Enterprise_Partnerships_April_2017_Full_Extent_Boundaries_in_England.shp'\n",
    "\n",
    "nuts_geo,leps_geo = [reverse_geocoder(places_df,path,'id') for path in [nuts_path,leps_path]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some quick observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(places_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nuts_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(leps_geo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of places that we aren't reverse geocoded because they are not in the UK, and more NUTS than LEPS because LEPS are only in England"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "We merge and save `nuts_geo` and `leps_geo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_geo = pd.merge(nuts_geo.reset_index(drop=False)[['index','nuts218cd','nuts218nm']],leps_geo.reset_index(drop=False)[['index','lep17cd','lep17nm']],\n",
    "                   left_on='index',right_on='index',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_geo.rename(columns={'index':'location_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_geo.to_csv(f'../../data/processed/crunchbase/{today_str}_rev_geocoded_places',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
