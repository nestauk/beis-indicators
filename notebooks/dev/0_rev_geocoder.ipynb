{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse geocoding\n",
    "\n",
    "This is the prototype for a simple tool to bin entities into a geography based on their lats and lons.\n",
    "\n",
    "In addition to prototyping the binner and testing it with the places data from CrunchBase, we will use this script to download the shapefiles for NUTS2 and LEPS that we will be using in the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "from data_getters.core import get_engine\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO, BytesIO\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daps_data(table,connection,chunksize=1000):\n",
    "    '''\n",
    "    Utility function to get data from DAPS with less faff\n",
    "    \n",
    "    Args:\n",
    "        -table is the SQL table in DAPS that we are extracting\n",
    "        -connection is the database connection we are using\n",
    "        -Chunksize are the chunks to download\n",
    "    \n",
    "    Returns:\n",
    "        -A dataframe with the data we have collected\n",
    "    \n",
    "    '''\n",
    "    #Get chunks\n",
    "    chunks = pd.read_sql_table(table, connection, chunksize=chunksize)\n",
    "    \n",
    "    #Create df\n",
    "    df = pd.concat(chunks)\n",
    "    \n",
    "    #Return data\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapes(url,file_name,path='../../data/shapefiles/'):\n",
    "    '''\n",
    "    Utility function to extract and save a bunch of shapefiles from the ONS open geography portal\n",
    "    \n",
    "    Arguments:\n",
    "        url: url for the shapefile zip\n",
    "        file_name: name of the file where we want to extract the data\n",
    "    \n",
    "    '''\n",
    "    #Get the data\n",
    "    print(f'getting {file_name}...')\n",
    "    req = requests.get(url)\n",
    "    \n",
    "    #Parse the content\n",
    "    z = ZipFile(BytesIO(req.content))\n",
    "    \n",
    "    #Save\n",
    "    print(f'saving {file_name}...')\n",
    "    z.extractall(f'{path}{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CrunchBase data using DAPS\n",
    "\n",
    "my_config = '../../mysqldb_team.config'\n",
    "\n",
    "#Create connection with SQL\n",
    "con = get_engine(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df = get_daps_data('geographic_data',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>country_alpha_2</th>\n",
       "      <th>country_alpha_3</th>\n",
       "      <th>country_numeric</th>\n",
       "      <th>continent</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s-graveland_netherlands</td>\n",
       "      <td>'s-graveland</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NL</td>\n",
       "      <td>NLD</td>\n",
       "      <td>528</td>\n",
       "      <td>EU</td>\n",
       "      <td>52.246038</td>\n",
       "      <td>5.130486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'s-gravendeel_netherlands</td>\n",
       "      <td>'s-gravendeel</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NL</td>\n",
       "      <td>NLD</td>\n",
       "      <td>528</td>\n",
       "      <td>EU</td>\n",
       "      <td>51.767282</td>\n",
       "      <td>4.598556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'s-gravenhage_netherlands</td>\n",
       "      <td>'s-gravenhage</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NL</td>\n",
       "      <td>NLD</td>\n",
       "      <td>528</td>\n",
       "      <td>EU</td>\n",
       "      <td>52.074946</td>\n",
       "      <td>4.269680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'s-gravenzande_netherlands</td>\n",
       "      <td>'s-gravenzande</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NL</td>\n",
       "      <td>NLD</td>\n",
       "      <td>528</td>\n",
       "      <td>EU</td>\n",
       "      <td>51.996606</td>\n",
       "      <td>4.158805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'s-heerenberg_netherlands</td>\n",
       "      <td>'s-heerenberg</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NL</td>\n",
       "      <td>NLD</td>\n",
       "      <td>528</td>\n",
       "      <td>EU</td>\n",
       "      <td>51.881981</td>\n",
       "      <td>6.250319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id            city      country country_alpha_2  \\\n",
       "0    's-graveland_netherlands    's-graveland  Netherlands              NL   \n",
       "1   's-gravendeel_netherlands   's-gravendeel  Netherlands              NL   \n",
       "2   's-gravenhage_netherlands   's-gravenhage  Netherlands              NL   \n",
       "3  's-gravenzande_netherlands  's-gravenzande  Netherlands              NL   \n",
       "4   's-heerenberg_netherlands   's-heerenberg  Netherlands              NL   \n",
       "\n",
       "  country_alpha_3 country_numeric continent   latitude  longitude  done  \n",
       "0             NLD             528        EU  52.246038   5.130486     1  \n",
       "1             NLD             528        EU  51.767282   4.598556     1  \n",
       "2             NLD             528        EU  52.074946   4.269680     1  \n",
       "3             NLD             528        EU  51.996606   4.158805     1  \n",
       "4             NLD             528        EU  51.881981   6.250319     1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes\n",
    "\n",
    "Download shapefiles from the [Open Geography Portal](https://geoportal.statistics.gov.uk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_url = 'https://opendata.arcgis.com/datasets/48b6b85bb7ea43699ee85f4ecd12fd36_1.zip?outSR=%7B%22latestWkid%22%3A27700%2C%22wkid%22%3A27700%7D'\n",
    "\n",
    "leps_url = 'https://opendata.arcgis.com/datasets/d4d519d1d1a1455a9b82331228f77489_1.zip?outSR=%7B%22latestWkid%22%3A27700%2C%22wkid%22%3A27700%7D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting nuts_2_2018...\n",
      "saving nuts_2_2018...\n",
      "getting leps_2017...\n",
      "saving leps_2017...\n"
     ]
    }
   ],
   "source": [
    "for url,name in zip([nuts_url,leps_url],['nuts_2_2018','leps_2017']):\n",
    "    get_shapes(url,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write reverse geo_coder\n",
    "\n",
    "The reverse geocoder does a point of polygon merge between a df and a boundary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_geocoder(place_df,shape_path,place_id,\n",
    "                     coord_names= ['longitude','latitude']):\n",
    "    '''\n",
    "    The reverse geocoder takes a df with geographical coordinates and does a spatial merge with a shapefile.\n",
    "    \n",
    "    Args:\n",
    "        place_df (df). A dataframe where every row is an entity we want to reverse geocode\n",
    "        shape_path (str): The path for the shapefile (note we will need to project this to WGS84)\n",
    "        place_id (str): the name of the variable with the place id in place dfs\n",
    "        coord_names (list): Names for the lon and lat variables in the place_df\n",
    "        \n",
    "    Returns:\n",
    "        A spatially merged df with the location ids and their \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Read the shapefile    \n",
    "    print('Reading shapefile...')\n",
    "    \n",
    "    shape = gp.read_file(shape_path)\n",
    "    \n",
    "    #Change its projection so it can deal with lats and lons\n",
    "    shape = shape.to_crs({'init':'epsg:4326'})\n",
    "    \n",
    "    #Create a place_holder df (ho ho) where the index is the place id\n",
    "    place_holder = gp.GeoDataFrame(index=place_df[place_id],crs={'init':'epsg:4326'})\n",
    "    \n",
    "    #Create the geo field for spatial merge using Point\n",
    "    place_holder['geometry'] = [Point(x[coord_names[0]],x[coord_names[1]]) for rid,x in place_df.iterrows()]\n",
    "    \n",
    "    print('Joining...')\n",
    "    \n",
    "    #Spatial join: looks for points inside the polygons\n",
    "    joined = gp.sjoin(place_holder,shape,op='within')\n",
    "    \n",
    "    #Return the joined df\n",
    "    return(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading shapefile...\n",
      "Joining...\n",
      "Reading shapefile...\n",
      "Joining...\n"
     ]
    }
   ],
   "source": [
    "#We loop this function over different shapefiles\n",
    "nuts_path = '../../data/shapefiles/nuts_2_2018/NUTS_Level_2_January_2018_Full_Extent_Boundaries_in_the_United_Kingdom.shp'\n",
    "leps_path = '../../data/shapefiles/leps_2017/Local_Enterprise_Partnerships_April_2017_Full_Extent_Boundaries_in_England.shp'\n",
    "\n",
    "nuts_geo,leps_geo = [reverse_geocoder(places_df,path,'id') for path in [nuts_path,leps_path]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some quick observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30469\n"
     ]
    }
   ],
   "source": [
    "print(len(places_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3151\n"
     ]
    }
   ],
   "source": [
    "print(len(nuts_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2918\n"
     ]
    }
   ],
   "source": [
    "print(len(leps_geo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of places that we aren't reverse geocoded because they are not in the UK, and more NUTS than LEPS because LEPS are only in England"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "We merge and save `nuts_geo` and `leps_geo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_geo = pd.merge(nuts_geo.reset_index(drop=False)[['index','nuts218cd','nuts218nm']],leps_geo.reset_index(drop=False)[['index','lep17cd','lep17nm']],\n",
    "                   left_on='index',right_on='index',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>nuts218cd</th>\n",
       "      <th>nuts218nm</th>\n",
       "      <th>lep17cd</th>\n",
       "      <th>lep17nm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbots-langley_united-kingdom</td>\n",
       "      <td>UKH2</td>\n",
       "      <td>Bedfordshire and Hertfordshire</td>\n",
       "      <td>E37000017</td>\n",
       "      <td>Hertfordshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ampthill_united-kingdom</td>\n",
       "      <td>UKH2</td>\n",
       "      <td>Bedfordshire and Hertfordshire</td>\n",
       "      <td>E37000041</td>\n",
       "      <td>South East Midlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ardeley_united-kingdom</td>\n",
       "      <td>UKH2</td>\n",
       "      <td>Bedfordshire and Hertfordshire</td>\n",
       "      <td>E37000017</td>\n",
       "      <td>Hertfordshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arlesey_united-kingdom</td>\n",
       "      <td>UKH2</td>\n",
       "      <td>Bedfordshire and Hertfordshire</td>\n",
       "      <td>E37000041</td>\n",
       "      <td>South East Midlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ashwell_united-kingdom</td>\n",
       "      <td>UKH2</td>\n",
       "      <td>Bedfordshire and Hertfordshire</td>\n",
       "      <td>E37000017</td>\n",
       "      <td>Hertfordshire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           index nuts218cd                       nuts218nm  \\\n",
       "0  abbots-langley_united-kingdom      UKH2  Bedfordshire and Hertfordshire   \n",
       "1        ampthill_united-kingdom      UKH2  Bedfordshire and Hertfordshire   \n",
       "2         ardeley_united-kingdom      UKH2  Bedfordshire and Hertfordshire   \n",
       "3         arlesey_united-kingdom      UKH2  Bedfordshire and Hertfordshire   \n",
       "4         ashwell_united-kingdom      UKH2  Bedfordshire and Hertfordshire   \n",
       "\n",
       "     lep17cd              lep17nm  \n",
       "0  E37000017        Hertfordshire  \n",
       "1  E37000041  South East Midlands  \n",
       "2  E37000017        Hertfordshire  \n",
       "3  E37000041  South East Midlands  \n",
       "4  E37000017        Hertfordshire  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_geo.rename(columns={'index':'location_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_geo.to_csv(f'../../data/processed/crunchbase/{today_str}_rev_geocoded_places',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
